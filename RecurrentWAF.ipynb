{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from dataset import CSICDataset, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global constants\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device of execution -  mps\n"
     ]
    }
   ],
   "source": [
    "# This is how we select a GPU if it's available on your computer or in the Colab environment.\n",
    "print('Device of execution - ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/dataset.csv')\n",
    "df = CSICDataset.process_df(df)\n",
    "\n",
    "# The following two lines are used to load the indices of the training and validation sets\n",
    "train_indices = np.load('./dataset/train_indices.npy')\n",
    "val_indices = np.load('./dataset/val_indices.npy')\n",
    "\n",
    "# The following two lines are used to select the training and validation sets from the dataframe based on the indices loaded above\n",
    "train_data = df.loc[train_indices].reset_index(drop=True)\n",
    "val_data = df.loc[val_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CSICDataset(df=train_data, vocab_size=5000, min_frequency=1, tokenization_algorithm='bpe')\n",
    "train_vocab = train_dataset.vocab\n",
    "\n",
    "val_dataset = CSICDataset(df=val_data, vocab=train_vocab)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler   = RandomSampler(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation dataset sizes match!\n"
     ]
    }
   ],
   "source": [
    "# Check Dataset Lengths\n",
    "assert len(train_dataset) == 45319, \"Training Dataset is of incorrect size\"\n",
    "assert len(val_dataset) == 11330, \"Validation Dataset is of incorrect size\"\n",
    "\n",
    "print('Training and Validation dataset sizes match!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = train_vocab.pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
    "    # Batch is of the form List[Tuple(Features(tokenized_ids,...), Labels)]\n",
    "    sequences = [torch.tensor(sample[0]['tokenized_ids'], dtype=torch.long, device=device) for sample in batch]\n",
    "    padded_tokens = torch.nn.utils.rnn.pad_sequence(sequences=sequences,batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    labels = torch.tensor([sample[1] for sample in batch])\n",
    "\n",
    "    return padded_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([64, 199])\n",
      "y: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_iterator:\n",
    "    print(f'x: {x.shape}')\n",
    "    print(f'y: {y.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentWAF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rec_hidden_size, fc_hidden_size, recurrent_type='LSTM', dropout=None):\n",
    "        super(RecurrentWAF, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=PADDING_VALUE)\n",
    "        \n",
    "        match recurrent_type:\n",
    "            case 'LSTM':\n",
    "                self.recurrent = nn.LSTM(input_size=embedding_dim, hidden_size=rec_hidden_size, batch_first=True)\n",
    "            case 'RNN':\n",
    "                self.recurrent = nn.RNN(input_size=embedding_dim, hidden_size=rec_hidden_size, batch_first=True)\n",
    "            case 'GRU':\n",
    "                self.recurrent = nn.GRU(input_size=embedding_dim, hidden_size=rec_hidden_size, batch_first=True)\n",
    "            \n",
    "            case _:\n",
    "                raise TypeError(\"Unsupported Recurrent Layer Type received\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=rec_hidden_size, out_features=fc_hidden_size, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=fc_hidden_size, out_features=1, bias=True)\n",
    "        )\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rec_hidden_size = rec_hidden_size\n",
    "        self.fc_hidden_size = fc_hidden_size\n",
    "        self.recurrent_type = recurrent_type\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed = self.embed_input(input)\n",
    "        \n",
    "        if self.recurrent_type == 'RNN' or self.recurrent_type == 'GRU':\n",
    "            _, hidden = self.recurrent(embed)\n",
    "        else:\n",
    "            _, (hidden, cell) = self.recurrent(embed)\n",
    "\n",
    "        hidden = hidden.squeeze(dim=0)\n",
    "        out = self.activation(self.fc(hidden))\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def embed_input(self, input):\n",
    "        if self.dropout:\n",
    "            return self.dropout(self.embed(input))\n",
    "        else:\n",
    "            return self.embed(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_and_f1_score(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    This function takes in two numpy arrays and computes the accuracy and F1 score\n",
    "    between them. You can use the imported sklearn functions to do this.\n",
    "\n",
    "    Args:\n",
    "        y_true (list) : A 1D numpy array of ground truth labels\n",
    "        y_predicted (list) : A 1D numpy array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float) : The accuracy of the predictions\n",
    "        f1_score (float) : The F1 score of the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the accuracy\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "\n",
    "    # Get the F1 score\n",
    "    f1 = f1_score(y_true, y_predicted)\n",
    "\n",
    "    return accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, criterion, optimizer, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to train a model for one epoch.\n",
    "    :param model: The model to be trained\n",
    "    :param criterion: The loss function\n",
    "    :param optim: The optimizer\n",
    "    :param iterator: The training data iterator\n",
    "    :return: The average loss for this epoch for all batches\n",
    "    \"\"\"\n",
    "    # Set the model to train mode (build computation graph)\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Training Model\"):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outs = model(x).squeeze(dim=-1)\n",
    "\n",
    "        loss = criterion(outs, y.float())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(iterator)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the validation set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Don't calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(iterator, total=len(iterator), desc=\"Validating Model\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            outs = model(x)\n",
    "            outs = outs.squeeze(dim=-1)\n",
    "\n",
    "            predictions = [True if out >= 0.5 else False for out in outs]\n",
    "            labels = [True if label == 1 else False for label in y]\n",
    "\n",
    "            loss = criterion(outs, y.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Add the predictions and labels to the lists\n",
    "            pred.extend(predictions)\n",
    "            true.extend(labels)\n",
    "        average_loss = total_loss / len(iterator)\n",
    "\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS:\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "REC_HIDDEN_DIM = 64\n",
    "FC_HIDDEN_DIM = 32\n",
    "REC_LAYER_TYPE = 'LSTM'\n",
    "DROPOUT = 0.1\n",
    "\n",
    "BETAS = (0.9,0.999)\n",
    "LR = 1e-4\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentWAF(vocab_size=len(train_vocab),embedding_dim=EMBEDDING_DIM,\n",
    "                     rec_hidden_size=REC_HIDDEN_DIM, fc_hidden_size=FC_HIDDEN_DIM,\n",
    "                     recurrent_type=REC_LAYER_TYPE, dropout=DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr=LR, betas=BETAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:05<00:00, 10.88it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:14<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Train_Loss: 0.6852185000653663 -- Val_Loss: 0.682008421153165 -- Val_Accuracy: 0.5684907325684024 -- Val_F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:04<00:00, 10.91it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:14<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 -- Train_Loss: 0.6789916478023879 -- Val_Loss: 0.6730516790673974 -- Val_Accuracy: 0.5684907325684024 -- Val_F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:09<00:00, 10.27it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:15<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 -- Train_Loss: 0.6646653217052034 -- Val_Loss: 0.6526645522439078 -- Val_Accuracy: 0.5684907325684024 -- Val_F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:08<00:00, 10.36it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:14<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 -- Train_Loss: 0.6349879104344566 -- Val_Loss: 0.6551228120420756 -- Val_Accuracy: 0.6643424536628421 -- Val_F1: 0.6062739414018015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:06<00:00, 10.63it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:14<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 -- Train_Loss: 0.6248709489663666 -- Val_Loss: 0.6392389244577857 -- Val_Accuracy: 0.6685789938217123 -- Val_F1: 0.5665473854322983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:09<00:00, 10.23it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:15<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 -- Train_Loss: 0.6141146163341861 -- Val_Loss: 0.5784137320987294 -- Val_Accuracy: 0.6581641659311562 -- Val_F1: 0.7054528861510381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:05<00:00, 10.78it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:15<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 -- Train_Loss: 0.6250883101240031 -- Val_Loss: 0.6210690805416429 -- Val_Accuracy: 0.68261253309797 -- Val_F1: 0.6257285595337219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:09<00:00, 10.20it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:19<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 -- Train_Loss: 0.6228632140058724 -- Val_Loss: 0.6259712670626265 -- Val_Accuracy: 0.6643424536628421 -- Val_F1: 0.6532956513811651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:14<00:00,  9.50it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:15<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 -- Train_Loss: 0.6675716506577345 -- Val_Loss: 0.6773813591244515 -- Val_Accuracy: 0.5849073256840247 -- Val_F1: 0.21183174124350596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:17<00:00,  9.13it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:17<00:00, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 -- Train_Loss: 0.6701094345918663 -- Val_Loss: 0.6447695939058669 -- Val_Accuracy: 0.6119152691968226 -- Val_F1: 0.3570697470390408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
    "    true, pred, val_loss = val_loop(model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'./models/rnn_waf.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Model: 100%|██████████| 178/178 [00:16<00:00, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Accuracy: 0.6120035304501323\n",
      "Final Validation F1-Score: 0.35730994152046786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "true, pred, val_loss = val_loop(model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural_language",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
