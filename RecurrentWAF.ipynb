{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from dataset import CSICDataset, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global constants\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device of execution -  mps\n"
     ]
    }
   ],
   "source": [
    "# This is how we select a GPU if it's available on your computer or in the Colab environment.\n",
    "print('Device of execution - ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/dataset.csv')\n",
    "df = CSICDataset.process_df(df)\n",
    "\n",
    "# The following two lines are used to load the indices of the training and validation sets\n",
    "train_indices = np.load('./dataset/train_indices.npy')\n",
    "val_indices = np.load('./dataset/val_indices.npy')\n",
    "\n",
    "# The following two lines are used to select the training and validation sets from the dataframe based on the indices loaded above\n",
    "train_data = df.loc[train_indices].reset_index(drop=True)\n",
    "val_data = df.loc[val_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CSICDataset(df=train_data, vocab_size=5000, min_frequency=1, tokenization_algorithm='bpe')\n",
    "train_vocab = train_dataset.vocab\n",
    "\n",
    "val_dataset = CSICDataset(df=val_data, vocab=train_vocab)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler   = RandomSampler(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Validation dataset sizes match!\n"
     ]
    }
   ],
   "source": [
    "# Check Dataset Lengths\n",
    "assert len(train_dataset) == 45319, \"Training Dataset is of incorrect size\"\n",
    "assert len(val_dataset) == 11330, \"Validation Dataset is of incorrect size\"\n",
    "\n",
    "print('Training and Validation dataset sizes match!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = train_vocab.pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, padding_value=PADDING_VALUE):\n",
    "    # Batch is of the form List[Tuple(Features(tokenized_ids,...), Labels)]\n",
    "    sequences = [torch.tensor(sample[0]['tokenized_ids'], dtype=torch.long, device=device) for sample in batch]\n",
    "    padded_tokens = torch.nn.utils.rnn.pad_sequence(sequences=sequences,batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    labels = torch.tensor([sample[1] for sample in batch])\n",
    "\n",
    "    return padded_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([64, 199])\n",
      "y: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_iterator:\n",
    "    print(f'x: {x.shape}')\n",
    "    print(f'y: {y.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentWAF(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, rec_hidden_size, fc_hidden_size, recurrent_type='LSTM', dropout=None):\n",
    "        super(RecurrentWAF, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=PADDING_VALUE)\n",
    "        \n",
    "        match recurrent_type:\n",
    "            case 'LSTM':\n",
    "                self.recurrent = nn.LSTM(input_size=embedding_dim, hidden_size=rec_hidden_size, batch_first=True)\n",
    "            case 'RNN':\n",
    "                self.recurrent = nn.RNN(input_size=embedding_dim, hidden_size=rec_hidden_size, batch_first=True)\n",
    "            case 'GRU':\n",
    "                self.recurrent = nn.GRU(input_size=embedding_dim, hidden_size=rec_hidden_size, batch_first=True)\n",
    "            \n",
    "            case _:\n",
    "                raise TypeError(\"Unsupported Recurrent Layer Type received\")\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=rec_hidden_size, out_features=fc_hidden_size, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=fc_hidden_size, out_features=1, bias=True)\n",
    "        )\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rec_hidden_size = rec_hidden_size\n",
    "        self.fc_hidden_size = fc_hidden_size\n",
    "        self.recurrent_type = recurrent_type\n",
    "\n",
    "    def forward(self, input):\n",
    "        embed = self.embed_input(input)\n",
    "        \n",
    "        if self.recurrent_type == 'RNN' or self.recurrent_type == 'GRU':\n",
    "            _, hidden = self.recurrent(embed)\n",
    "        else:\n",
    "            _, (hidden, cell) = self.recurrent(embed)\n",
    "\n",
    "        hidden = hidden.squeeze(dim=0)\n",
    "        out = self.activation(self.fc(hidden))\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def embed_input(self, input):\n",
    "        if self.dropout:\n",
    "            return self.dropout(self.embed(input))\n",
    "        else:\n",
    "            return self.embed(input)\n",
    "        \n",
    "    # @staticmethod\n",
    "    # def load(model_path: str):\n",
    "    #     params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    #     args = params['args']\n",
    "    #     model = RecurrentWAF(vocab=params['vocab'], **args)\n",
    "    #     model.load_state_dict(params['state_dict'])\n",
    "\n",
    "    #     return model\n",
    "\n",
    "    # def save(self, path: str):\n",
    "    #     print('save model parameters to [%s]' % path, file=sys.stderr)\n",
    "\n",
    "    #     params = {\n",
    "    #         'args': dict(embed_size=self.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate,\n",
    "    #                      label_smoothing=self.label_smoothing),\n",
    "    #         'vocab': self.vocab,\n",
    "    #         'state_dict': self.state_dict()\n",
    "    #     }\n",
    "\n",
    "    #     torch.save(params, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_and_f1_score(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    This function takes in two numpy arrays and computes the accuracy and F1 score\n",
    "    between them. You can use the imported sklearn functions to do this.\n",
    "\n",
    "    Args:\n",
    "        y_true (list) : A 1D numpy array of ground truth labels\n",
    "        y_predicted (list) : A 1D numpy array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float) : The accuracy of the predictions\n",
    "        f1_score (float) : The F1 score of the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the accuracy\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "\n",
    "    # Get the F1 score\n",
    "    f1 = f1_score(y_true, y_predicted)\n",
    "\n",
    "    return accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, criterion, optimizer, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to train a model for one epoch.\n",
    "    :param model: The model to be trained\n",
    "    :param criterion: The loss function\n",
    "    :param optim: The optimizer\n",
    "    :param iterator: The training data iterator\n",
    "    :return: The average loss for this epoch for all batches\n",
    "    \"\"\"\n",
    "    # Set the model to train mode (build computation graph)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Training Model\"):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outs = model(x).squeeze(dim=-1)\n",
    "\n",
    "        loss = criterion(outs, y.float())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(iterator)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the validation set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    total_comp_time = 0\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Don't calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(iterator, total=len(iterator), desc=\"Validating Model\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            start = time.time()\n",
    "            outs = model(x)\n",
    "            total_comp_time += time.time() - start\n",
    "            \n",
    "            outs = outs.squeeze(dim=-1)\n",
    "\n",
    "            predictions = [True if out >= 0.5 else False for out in outs]\n",
    "            labels = [True if label == 1 else False for label in y]\n",
    "\n",
    "            loss = criterion(outs, y.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Add the predictions and labels to the lists\n",
    "            pred.extend(predictions)\n",
    "            true.extend(labels)\n",
    "        average_loss = total_loss / len(iterator)\n",
    "        average_comp_time = total_comp_time / len(iterator)\n",
    "\n",
    "    return true, pred, average_loss, average_comp_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS:\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "REC_HIDDEN_DIM = 64\n",
    "FC_HIDDEN_DIM = 64\n",
    "REC_LAYER_TYPE = 'LSTM'\n",
    "DROPOUT = 0.2\n",
    "\n",
    "BETAS = (0.9,0.999)\n",
    "LR = 1e-3\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentWAF(vocab_size=len(train_vocab),embedding_dim=EMBEDDING_DIM,\n",
    "                     rec_hidden_size=REC_HIDDEN_DIM, fc_hidden_size=FC_HIDDEN_DIM,\n",
    "                     recurrent_type=REC_LAYER_TYPE, dropout=DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr=LR, betas=BETAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:08<00:00, 10.33it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:17<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Train_Loss: 0.6792557511074082 -- Val_Loss: 0.6720932340354062 -- Val_Accuracy: 0.5684907325684024 -- Val_F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:12<00:00,  9.83it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:16<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 -- Train_Loss: 0.5240518565490653 -- Val_Loss: 0.40687665962771086 -- Val_Accuracy: 0.7902912621359224 -- Val_F1: 0.8012048192771084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:19<00:00,  8.96it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:20<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 -- Train_Loss: 0.38124701969209945 -- Val_Loss: 0.3413784425245242 -- Val_Accuracy: 0.8528684907325684 -- Val_F1: 0.8415549852675601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:18<00:00,  9.04it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:18<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 -- Train_Loss: 0.31088578585950377 -- Val_Loss: 0.2851855932714305 -- Val_Accuracy: 0.8664607237422771 -- Val_F1: 0.8456909739928608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:18<00:00,  8.98it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:16<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 -- Train_Loss: 0.2711170658006823 -- Val_Loss: 0.24995146217766437 -- Val_Accuracy: 0.8997352162400706 -- Val_F1: 0.8937125748502994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:14<00:00,  9.46it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:14<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 -- Train_Loss: 0.2387723523298507 -- Val_Loss: 0.19953990409850847 -- Val_Accuracy: 0.923477493380406 -- Val_F1: 0.9167066961283504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:14<00:00,  9.56it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:13<00:00, 13.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 -- Train_Loss: 0.1890005446150794 -- Val_Loss: 0.1807700813737478 -- Val_Accuracy: 0.9327449249779347 -- Val_F1: 0.925904317386231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:14<00:00,  9.48it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:16<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 -- Train_Loss: 0.17242909855982985 -- Val_Loss: 0.17275825446325072 -- Val_Accuracy: 0.9378640776699029 -- Val_F1: 0.9322164452147121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:15<00:00,  9.45it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:15<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 -- Train_Loss: 0.16348104615274117 -- Val_Loss: 0.17686632340460012 -- Val_Accuracy: 0.9318623124448367 -- Val_F1: 0.926657799733992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 709/709 [01:16<00:00,  9.22it/s]\n",
      "Validating Model: 100%|██████████| 178/178 [00:15<00:00, 11.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 -- Train_Loss: 0.15568438897957587 -- Val_Loss: 0.1611709658244855 -- Val_Accuracy: 0.9423654015887025 -- Val_F1: 0.9371087354329192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_loop(model, criterion, optimizer, train_iterator)\n",
    "    true, pred, val_loss, comp_time = val_loop(model, criterion, val_iterator)\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'./models/lstm_waf_new.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Model: 100%|██████████| 178/178 [00:15<00:00, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Accuracy: 0.9029126213592233\n",
      "Final Validation F1-Score: 0.8972154737432255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "true, pred, val_loss, comp_time = val_loop(model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./models/lstm_waf_new.bin', map_location='mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Unstructured Pruning: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.recurrent, 'weight_ih_l0'),\n",
    "    (model.recurrent, 'weight_hh_l0'),\n",
    "    (model.recurrent, 'bias_ih_l0'),\n",
    "    (model.recurrent, 'bias_hh_l0'),\n",
    "    (model.fc[0], 'weight'),\n",
    "    (model.fc[0], 'bias'),\n",
    "    (model.fc[2], 'weight'),\n",
    "    (model.fc[2], 'bias'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters=parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")\n",
    "\n",
    "# Apply pruning to the model\n",
    "# prune.remove(model.recurrent, 'weight_ih_l0')\n",
    "# prune.remove(model.recurrent, 'weight_hh_l0')\n",
    "# prune.remove(model.recurrent, 'bias_ih_l0')\n",
    "# prune.remove(model.recurrent, 'bias_hh_l0')\n",
    "# prune.remove(model.fc[0], 'weight')\n",
    "# prune.remove(model.fc[0], 'bias')\n",
    "# prune.remove(model.fc[2], 'weight')\n",
    "# prune.remove(model.fc[2], 'bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Structured Pruning: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.recurrent, 'weight_ih_l0'),\n",
    "    (model.recurrent, 'weight_hh_l0'),\n",
    "    # (model.recurrent, 'bias_ih_l0'),\n",
    "    # (model.recurrent, 'bias_hh_l0'),\n",
    "    # (model.fc[0], 'weight'),\n",
    "    # (model.fc[0], 'bias'),\n",
    "    (model.fc[2], 'weight'),\n",
    "    # (model.fc[2], 'bias'),\n",
    ")\n",
    "\n",
    "for module, name in parameters_to_prune:\n",
    "    prune.ln_structured(\n",
    "        module=module,\n",
    "        name=name,\n",
    "        amount=0.2,\n",
    "        n=2,\n",
    "        dim=1\n",
    "    )\n",
    "    prune.remove(module=module, name=name)\n",
    "\n",
    "# Apply pruning to the model\n",
    "# prune.remove(model.recurrent, 'weight_ih_l0')\n",
    "# prune.remove(model.recurrent, 'weight_hh_l0')\n",
    "# prune.remove(model.recurrent, 'bias_ih_l0')\n",
    "# prune.remove(model.recurrent, 'bias_hh_l0')\n",
    "# prune.remove(model.fc[0], 'weight')\n",
    "# prune.remove(model.fc[0], 'bias')\n",
    "# prune.remove(model.fc[2], 'weight')\n",
    "# prune.remove(model.fc[2], 'bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Sparsity After Pruning: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in recurrent.weight_ih_l0': 20.31%\n",
      "Sparsity in recurrent.weight_hh_l0: 20.31%\n",
      "Sparsity in recurrent.bias_ih_l0': 0.00%\n",
      "Sparsity in recurrent.bias_hh_l0: 0.00%\n",
      "Sparsity in fc1.weight: 0.00%\n",
      "Sparsity in fc1.bias: 0.00%\n",
      "Sparsity in fc2.weight: 40.62%\n",
      "Sparsity in fc2.bias: 0.00%\n",
      "Global sparsity: 18.98%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sparsity in recurrent.weight_ih_l0': {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.recurrent.weight_ih_l0 == 0))\n",
    "        / float(model.recurrent.weight_ih_l0.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in recurrent.weight_hh_l0: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.recurrent.weight_hh_l0 == 0))\n",
    "        / float(model.recurrent.weight_hh_l0.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in recurrent.bias_ih_l0': {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.recurrent.bias_ih_l0 == 0))\n",
    "        / float(model.recurrent.bias_ih_l0.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in recurrent.bias_hh_l0: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.recurrent.bias_hh_l0 == 0))\n",
    "        / float(model.recurrent.bias_hh_l0.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc[0].weight == 0))\n",
    "        / float(model.fc[0].weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc1.bias: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc[0].bias == 0))\n",
    "        / float(model.fc[0].bias.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc[2].weight == 0))\n",
    "        / float(model.fc[2].weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc2.bias: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc[2].bias == 0))\n",
    "        / float(model.fc[2].bias.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.recurrent.bias_ih_l0 == 0)\n",
    "            + torch.sum(model.recurrent.weight_ih_l0 == 0)\n",
    "            + torch.sum(model.recurrent.weight_hh_l0 == 0)\n",
    "            + torch.sum(model.recurrent.bias_hh_l0 == 0)\n",
    "            + torch.sum(model.fc[0].weight == 0)\n",
    "            + torch.sum(model.fc[2].weight == 0)\n",
    "            + torch.sum(model.fc[0].bias == 0)\n",
    "            + torch.sum(model.fc[2].bias == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.recurrent.bias_ih_l0.nelement()\n",
    "            + model.recurrent.bias_hh_l0.nelement()\n",
    "            + model.recurrent.weight_ih_l0.nelement()\n",
    "            + model.recurrent.weight_hh_l0.nelement()\n",
    "            + model.fc[0].weight.nelement()\n",
    "            + model.fc[0].bias.nelement()\n",
    "            + model.fc[2].weight.nelement()\n",
    "            + model.fc[2].bias.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Model: 100%|██████████| 178/178 [00:16<00:00, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Accuracy: 0.9326566637246249\n",
      "Final Validation F1-Score: 0.9270624223305611\n",
      "Average Time to Compute Forward Pass: 4.835521237234052 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "true, pred, val_loss, comp_time = val_loop(model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")\n",
    "print(f\"Average Time to Compute Forward Pass: {comp_time*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Model: 100%|██████████| 178/178 [00:19<00:00,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Accuracy: 0.9494263018534863\n",
      "Final Validation F1-Score: 0.9443310988050131\n",
      "Average Time to Compute Forward Pass: 11.048862103665813 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "true, pred, val_loss, comp_time = val_loop(model, criterion, val_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Validation Accuracy: {accuracy}\")\n",
    "print(f\"Final Validation F1-Score: {f1}\")\n",
    "print(f\"Average Time to Compute Forward Pass: {comp_time*1000} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural_language",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
