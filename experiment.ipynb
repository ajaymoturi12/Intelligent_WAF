{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligent Web Application Firewall\n",
    "====================================\n",
    "This project is based on data from the ECML/PKDD 2007 Challenge and CSIC 2010 Dataset, [available on GitHub](https://github.com/msudol/Web-Application-Attack-Datasets/blob/master/CSVData/csic_ecml_normalized_final.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure\n",
    "Initially setup this experiment. For the sake of making setup faster, we will go ahead and download the dataset if it does not already exist on our system. For now, we'll download it off of [msudol/Web-Application-Attack-Datasets](https://github.com/msudol/Web-Application-Attack-Datasets) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://raw.githubusercontent.com/msudol/Web-Application-Attack-Datasets/master/CSVData/csic_ecml_normalized_final.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Data Caching\n",
    "We're going to cache the pre-processed data to avoid having to process it repeatedly. Set the `NORMALIZATION_VERSION` below accordingly to use a particular version of the normalized dataset. If it is set to `None`, we'll assume that pre-processing is still a work in progress and to continuously do pre-processing again (you should probably change this eventually). Note that this value will get automatically overridden if set to `None`, so you'll need to run these blocks again if you want data to be pre-processed again!\n",
    "\n",
    "Note that even if there's a cached version available, we'll always keep a version of the original data (kinda just for the sake of it).\n",
    "\n",
    "Also, if you *really* hate caching, just turn off saving..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION_VERSION = None\n",
    "SAVE_NORMALIZED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get things done! This code block will automatically download the dataset if needed and set up our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅] Dataset Directory Exists\n",
      "[✅] Normalized Dataset Directory Exists\n",
      "[✅] Dataset Exists\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "DATASET_DIRECTORY_PATH = pathlib.Path('dataset')\n",
    "DATASET_NORMALIZED_DIRECTORY_PATH = DATASET_DIRECTORY_PATH.joinpath('normalized')\n",
    "DATASET_RAW_PATH = DATASET_DIRECTORY_PATH.joinpath('dataset.csv')\n",
    "\n",
    "\n",
    "if DATASET_DIRECTORY_PATH.is_dir():\n",
    "    print('[✅] Dataset Directory Exists')\n",
    "else:\n",
    "    print('[~] Creating Dataset Directory')\n",
    "    DATASET_DIRECTORY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DATASET_NORMALIZED_DIRECTORY_PATH.is_dir():\n",
    "    print('[✅] Normalized Dataset Directory Exists')\n",
    "else:\n",
    "    print('[~] Creating Normalized Dataset Directory')\n",
    "    DATASET_NORMALIZED_DIRECTORY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DATASET_RAW_PATH.is_file():\n",
    "    print('[✅] Dataset Exists')\n",
    "else:\n",
    "    print(f'[~] Downloading Dataset')\n",
    "    urllib.request.urlretrieve(DATASET_URL, DATASET_RAW_PATH)\n",
    "\n",
    "    if DATASET_RAW_PATH.is_file():\n",
    "        print(f'[✅] Dataset Available at {DATASET_RAW_PATH}')\n",
    "    else:\n",
    "        print('[⚠️] Failed to Download Dataset')\n",
    "        raise SystemExit('Dataset Download Failure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets figure out our caching situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_cache_path: pathlib.Path = None\n",
    "if NORMALIZATION_VERSION is None:\n",
    "    # Figure out the next normalization version...\n",
    "    i = 0\n",
    "    while True:\n",
    "        proposed_path = DATASET_NORMALIZED_DIRECTORY_PATH.joinpath(f'{i}.csv')\n",
    "        if not proposed_path.exists():\n",
    "            effective_cache_path = proposed_path\n",
    "            NORMALIZATION_VERSION = i\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "else:\n",
    "    effective_cache_path = DATASET_NORMALIZED_DIRECTORY_PATH.joinpath(f'{int(NORMALIZATION_VERSION)}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching path has been figured out! Time to actually normalize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENUMERATIONS = {\n",
    "    'Class': {'Valid': 0, 'Anomalous': 1},\n",
    "    'Method': {'GET': 0, 'POST': 1, 'PUT': 2},\n",
    "    'Host-Header': {'HTTP/1.0': 0, 'HTTP/1.1': 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def enumerate(df):\n",
    "    for field, enumeration in ENUMERATIONS.items():\n",
    "        print(f'[~] Processing Field: {field}')\n",
    "        if field not in df:\n",
    "            raise RuntimeWarning(f'Field {field} Does Not Exist')\n",
    "        \n",
    "        unenumerated_values = set(df[field].unique()).difference(set(enumeration.keys()))\n",
    "        for unenumerated_value in unenumerated_values:\n",
    "            raise RuntimeWarning(f'Failed to Enumerate Value \"{unenumerated_value}\" for Field {field}')\n",
    "        \n",
    "        df[field] = df[field].map(enumeration)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def preprocess(path):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    df = enumerate(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[~] Processing Field: Class\n",
      "[~] Processing Field: Method\n",
      "[~] Processing Field: Host-Header\n"
     ]
    },
    {
     "ename": "RuntimeWarning",
     "evalue": "Failed to Enumerate Value \"HTTP/1.1\" for Field Host-Header",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(effective_cache_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# We're going to need to do some pre-processing!\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_RAW_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SAVE_NORMALIZED:\n\u001b[1;32m      9\u001b[0m         data\u001b[38;5;241m.\u001b[39mto_csv(effective_cache_path)\n",
      "Cell \u001b[0;32mIn[136], line 20\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(path):\n\u001b[1;32m     18\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(path)\n\u001b[0;32m---> 20\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[136], line 11\u001b[0m, in \u001b[0;36menumerate\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m     unenumerated_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(df[field]\u001b[38;5;241m.\u001b[39munique())\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mset\u001b[39m(enumeration\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m unenumerated_value \u001b[38;5;129;01min\u001b[39;00m unenumerated_values:\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to Enumerate Value \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munenumerated_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for Field \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     df[field] \u001b[38;5;241m=\u001b[39m df[field]\u001b[38;5;241m.\u001b[39mmap(enumeration)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mRuntimeWarning\u001b[0m: Failed to Enumerate Value \"HTTP/1.1\" for Field Host-Header"
     ]
    }
   ],
   "source": [
    "data: pd.DataFrame = None\n",
    "if effective_cache_path.is_file():\n",
    "    data = pd.read_csv(effective_cache_path)\n",
    "else:\n",
    "    # We're going to need to do some pre-processing!\n",
    "    data = preprocess(DATASET_RAW_PATH)\n",
    "\n",
    "    if SAVE_NORMALIZED:\n",
    "        data.to_csv(effective_cache_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
