{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligent Web Application Firewall\n",
    "====================================\n",
    "This project is based on data from the ECML/PKDD 2007 Challenge and CSIC 2010 Dataset, [available on GitHub](https://github.com/msudol/Web-Application-Attack-Datasets/blob/master/CSVData/csic_ecml_normalized_final.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure\n",
    "Initially setup this experiment. For the sake of making setup faster, we will go ahead and download the dataset if it does not already exist on our system. For now, we'll download it off of [msudol/Web-Application-Attack-Datasets](https://github.com/msudol/Web-Application-Attack-Datasets) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://raw.githubusercontent.com/msudol/Web-Application-Attack-Datasets/master/CSVData/csic_ecml_normalized_final.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Data Caching\n",
    "We're going to cache the pre-processed data to avoid having to process it repeatedly. Set the `NORMALIZATION_VERSION` below accordingly to use a particular version of the normalized dataset. If it is set to `None`, we'll assume that pre-processing is still a work in progress and to continuously do pre-processing again (you should probably change this eventually). Note that this value will get automatically overridden if set to `None`, so you'll need to run these blocks again if you want data to be pre-processed again!\n",
    "\n",
    "Note that even if there's a cached version available, we'll always keep a version of the original data (kinda just for the sake of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION_VERSION = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get things done! This code block will automatically download the dataset if needed and set up our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅] Dataset Directory Exists\n",
      "[✅] Normalized Dataset Directory Exists\n",
      "[✅] Dataset Exists\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "DATASET_DIRECTORY_PATH = pathlib.Path('dataset')\n",
    "DATASET_NORMALIZED_DIRECTORY_PATH = DATASET_DIRECTORY_PATH.joinpath('normalized')\n",
    "DATASET_RAW_PATH = DATASET_DIRECTORY_PATH.joinpath('dataset.csv')\n",
    "\n",
    "\n",
    "if DATASET_DIRECTORY_PATH.is_dir():\n",
    "    print('[✅] Dataset Directory Exists')\n",
    "else:\n",
    "    print('[~] Creating Dataset Directory')\n",
    "    DATASET_DIRECTORY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DATASET_NORMALIZED_DIRECTORY_PATH.is_dir():\n",
    "    print('[✅] Normalized Dataset Directory Exists')\n",
    "else:\n",
    "    print('[~] Creating Normalized Dataset Directory')\n",
    "    DATASET_NORMALIZED_DIRECTORY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DATASET_RAW_PATH.is_file():\n",
    "    print('[✅] Dataset Exists')\n",
    "else:\n",
    "    print(f'[~] Downloading Dataset')\n",
    "    urllib.request.urlretrieve(DATASET_URL, DATASET_RAW_PATH)\n",
    "\n",
    "    if DATASET_RAW_PATH.is_file():\n",
    "        print(f'[✅] Dataset Available at {DATASET_RAW_PATH}')\n",
    "    else:\n",
    "        print('[⚠️] Failed to Download Dataset')\n",
    "        raise SystemExit('Dataset Download Failure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets figure out our caching situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_cache_path: pathlib.Path = None\n",
    "if NORMALIZATION_VERSION is None:\n",
    "    # Figure out the next normalization version...\n",
    "    i = 0\n",
    "    while True:\n",
    "        proposed_path = DATASET_NORMALIZED_DIRECTORY_PATH.joinpath(f'{i}.csv')\n",
    "        if not proposed_path.exists():\n",
    "            effective_cache_path = proposed_path\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "else:\n",
    "    effective_cache_path = DATASET_NORMALIZED_DIRECTORY_PATH.joinpath(f'{int(NORMALIZATION_VERSION)}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching path has been figured out! Time to actually normalize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# We're going to need to do some pre-processing!\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     data \u001b[38;5;241m=\u001b[39m preprocess(DATASET_RAW_PATH)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(effective_cache_path)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "data: pd.DataFrame = None\n",
    "if effective_cache_path.is_file():\n",
    "    data = pd.read_csv(effective_cache_path)\n",
    "else:\n",
    "    # We're going to need to do some pre-processing!\n",
    "    data = preprocess(DATASET_RAW_PATH)\n",
    "    data.to_csv(effective_cache_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
