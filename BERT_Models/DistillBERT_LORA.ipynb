{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":5077,"status":"ok","timestamp":1714147930124,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"XFbiyvdlGN9u"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset, RandomSampler\n","\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import random"]},{"cell_type":"markdown","metadata":{"id":"tYR1S8cqKNLm"},"source":["# CSIC and Vocab Classes"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1714147930542,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"S65-90NlKMll"},"outputs":[],"source":["import os\n","from typing import List\n","from collections import Counter\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import ByteLevel\n","\n","from urllib import parse\n","\n","class CSICDataset(Dataset):\n","    def __init__(self, df: pd.DataFrame, vocab=None, vocab_size=1000, min_frequency=1, special_tokens=[\"[UNK]\",\"[CLS]\",\"[PAD]\"], tokenization_algorithm=\"bpe\"):\n","        self.df = df\n","\n","        # Export text content to csv for learning tokenization; apply BPE\n","        path = os.path.join('.', 'tokenization_input')\n","\n","        self.df.to_csv(path_or_buf=path, columns=['content_for_tokenization'], index=False, header=False)\n","\n","        if vocab == None:\n","            vocab = Vocab(vocab_size=vocab_size, min_frequency=min_frequency,\n","                           special_tokens=special_tokens,\n","                           tokenization_algorithm=tokenization_algorithm)\n","            vocab.build(corpus_files=[path])\n","\n","        self.vocab = vocab\n","\n","        self.encode_df()\n","\n","\n","    @staticmethod\n","    def process_df(df: pd.DataFrame):\n","        # Pre-process data by dropping rows without POST-Data or GET-Query\n","        get_mask, post_mask = df['GET-Query'].notna(), df['POST-Data'].notna()\n","\n","        df.loc[get_mask,\"content_for_tokenization\"] = df.loc[get_mask,\"GET-Query\"]\n","        df.loc[post_mask,\"content_for_tokenization\"] = df.loc[post_mask,\"POST-Data\"]\n","\n","        df = df[get_mask | post_mask]\n","        df = df.drop(columns=[\"GET-Query\",\"POST-Data\", \"Accept-Charset\", \"Accept-Language\", \"Accept\", \"Cache-control\", \"Pragma\", \"Content-Type\", \"Host-Header\", \"Connection\"])\n","\n","        return df\n","\n","    def encode_df(self):\n","        # Tokenize the GET-Query and POST-Data columns according to the subword vocabulary learned from BPE\n","        self.df[\"tokenized_ids\"] = self.df[\"content_for_tokenization\"].apply(lambda x: self.vocab.words2indices(x))\n","        self.df[\"tokenized\"] = self.df[\"content_for_tokenization\"].apply(lambda x: self.vocab.tokenize(x))\n","        self.df = self.df.drop(columns=[\"content_for_tokenization\"])\n","\n","        self.class_encoder, self.method_encoder = LabelEncoder(), LabelEncoder()\n","        self.df['Class'], self.df['Method'] = self.class_encoder.fit_transform(self.df['Class']), self.class_encoder.fit_transform(self.df['Method'])\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        features = self.df.iloc[index].drop(['Class', 'User-Agent'])\n","        label = self.df.iloc[index]['Class']\n","\n","        return features, label"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":163,"status":"ok","timestamp":1714147931893,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"-Rj6eSuvKWg_"},"outputs":[],"source":["class Vocab(object):\n","    def __init__(self, vocab_size=0, min_frequency=0, special_tokens: List[str]=[], unk_token=\"[UNK]\", pad_token=\"[PAD]\", tokenizer=None, tokenization_algorithm=\"bpe\"):\n","        if tokenizer:\n","            self.tokenizer = tokenizer\n","\n","            self.word2id = tokenizer.get_vocab()\n","            self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","            self.unk_id = self.word2id[unk_token]\n","\n","        else:\n","            assert vocab_size > 0\n","            assert min_frequency > 0\n","\n","            self.vocab_size = vocab_size\n","            self.min_frequency = min_frequency\n","            self.special_tokens = special_tokens\n","            self.unk_token = unk_token\n","            self.pad_token = pad_token\n","            self.tokenization_algorithm = tokenization_algorithm\n","\n","    def build(self, corpus_files: List[str]):\n","        if self.tokenization_algorithm == 'bpe':\n","            tokenizer = Tokenizer(BPE(unk_token=self.unk_token))\n","            trainer = BpeTrainer(vocab_size=self.vocab_size, min_frequency=self.min_frequency, special_tokens=self.special_tokens)\n","            tokenizer.pre_tokenizer = ByteLevel()\n","\n","            tokenizer.train(corpus_files, trainer)\n","\n","            self.tokenizer = tokenizer\n","            self.word2id = tokenizer.get_vocab()\n","            self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","        elif self.tokenization_algorithm == 'vocab_map':\n","            self.word2id, self.id2word = dict(), dict()\n","            curr_id, self.min_frequency = 1,1\n","            counter = Counter()\n","\n","            def add_to_vocab(token: str, ignore_cutoff=False):\n","                nonlocal curr_id\n","                if (ignore_cutoff or counter[token] >= self.min_frequency) and token not in self.word2id:\n","                    self.word2id[token] = curr_id\n","                    self.id2word[curr_id] = token\n","\n","                    curr_id += 1\n","\n","            for file_path in corpus_files:\n","                with open(file_path, 'r') as file:\n","                    for line in file:\n","                        tokens = Vocab.parse_req_body_or_params(line)\n","                        counter.update(tokens)\n","\n","            unwanted_tokens = [' ','']\n","            for token in unwanted_tokens:\n","                if token in counter:\n","                    del counter[token]\n","\n","            for token in self.special_tokens:\n","                add_to_vocab(token, ignore_cutoff=True)\n","\n","            for token in set(counter.elements()):\n","                add_to_vocab(token)\n","\n","        else:\n","            raise TypeError(\"Unsupported tokenization algorithm detected\")\n","\n","        self.unk_id = self.word2id[self.unk_token]\n","        self.pad_id = self.word2id[self.pad_token]\n","\n","    def __getitem__(self, word):\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        return self.id2word[wid]\n","\n","    def save(self, file_path):\n","        self.tokenizer.save(path=file_path)\n","\n","    def words2indices(self, content):\n","        if self.tokenization_algorithm == 'bpe':\n","            if type(content) == list:\n","                return [self.tokenizer.encode(row).ids for row in content]\n","            else:\n","                return self.tokenizer.encode(content).ids\n","\n","        elif self.tokenization_algorithm == 'vocab_map':\n","            if type(content) == list:\n","                return [[self[token] for token in Vocab.parse_req_body_or_params(line)] for line in content]\n","            else:\n","                return [self[token] for token in Vocab.parse_req_body_or_params(content)]\n","        else:\n","            raise TypeError(\"Unsupported tokenization algorithm detected\")\n","\n","    def tokenize(self, content):\n","        if self.tokenization_algorithm == 'bpe':\n","            if type(content) == list:\n","                return [self.tokenizer.encode(row).tokens for row in content]\n","            else:\n","                return self.tokenizer.encode(content).tokens\n","\n","        elif self.tokenization_algorithm == 'vocab_map':\n","            if type(content) == list:\n","                return [[token if self.__contains__(token) else self.unk_token for token in Vocab.parse_req_body_or_params(line)] for line in content]\n","            else:\n","                return [token if self.__contains__(token) else self.unk_token for token in Vocab.parse_req_body_or_params(content)]\n","\n","        else:\n","            raise TypeError(\"Unsupported tokenization algorithm detected\")\n","\n","    @staticmethod\n","    def parse_req_body_or_params(line: str):\n","        parsed_line = parse.parse_qs(parse.unquote_plus(string=line))\n","\n","        tokens = []\n","        for k, v in parsed_line.items():\n","            tokens.append(k)\n","            tokens.extend(v)\n","\n","        return tokens\n","\n","    @staticmethod\n","    def load(file_path: str):\n","        return Vocab(tokenizer=Tokenizer.from_file(file_path))"]},{"cell_type":"markdown","metadata":{"id":"LLCCxrOzKdaN"},"source":["# Data Loading"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":154,"status":"ok","timestamp":1714147934684,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"Bi76zAr_JpbV"},"outputs":[],"source":["# Defining global constants\n","RANDOM_SEED = 42\n","BATCH_SIZE = 64\n","\n","torch.manual_seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1714147935494,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"FiQJ39fWKlBI","outputId":"cc6d9b7f-b2d2-46ca-acc6-ee5193ce8b84"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device of execution -  cpu\n"]}],"source":["# This is how we select a GPU if it's available on your computer or in the Colab environment.\n","print('Device of execution - ', device)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":171,"status":"ok","timestamp":1714147936853,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"_fsd7PRFKpPT"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./dataset.csv')\n","df = CSICDataset.process_df(df)\n","\n","# # The following two lines are used to load the indices of the training and validation sets\n","# train_indices = np.load('./dataset/train_indices.npy')\n","# val_indices = np.load('./dataset/val_indices.npy')\n","\n","# # The following two lines are used to select the training and validation sets from the dataframe based on the indices loaded above\n","# train_data = df.loc[train_indices].reset_index(drop=True)\n","# val_data = df.loc[val_indices].reset_index(drop=True)\n","\n","train_data, val_data = train_test_split(df, test_size=0.2)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":29332,"status":"ok","timestamp":1714147966654,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"u8E1liYAQ9yL"},"outputs":[],"source":["train_dataset = CSICDataset(df=train_data, vocab_size=5000, min_frequency=1, tokenization_algorithm='bpe')\n","train_vocab = train_dataset.vocab\n","\n","val_dataset = CSICDataset(df=val_data, vocab=train_vocab)\n","\n","train_sampler = RandomSampler(train_dataset)\n","val_sampler   = RandomSampler(val_dataset)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1714147966655,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"FbUOjJ8aROv4","outputId":"cee73a4d-1f03-4637-f6de-ba93db405846"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training and Validation dataset sizes match!\n"]}],"source":["# Check Dataset Lengths\n","assert len(train_dataset) == 45319, \"Training Dataset is of incorrect size\"\n","assert len(val_dataset) == 11330, \"Validation Dataset is of incorrect size\"\n","\n","print('Training and Validation dataset sizes match!')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714147966655,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"Ihg6xpQtRW5S"},"outputs":[],"source":["PADDING_VALUE = train_vocab.pad_id"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714147966655,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"uZnYLvVVRYgx"},"outputs":[],"source":["def collate_fn(batch, padding_value=PADDING_VALUE):\n","    # Batch is of the form List[Tuple(Features(tokenized_ids,...), Labels)]\n","    sequences = [torch.tensor(sample[0]['tokenized_ids'], dtype=torch.long, device=device) for sample in batch]\n","    padded_tokens = torch.nn.utils.rnn.pad_sequence(sequences=sequences,batch_first=True, padding_value=padding_value)\n","    attention_mask = (padded_tokens != padding_value).float()\n","    labels = torch.tensor([sample[1] for sample in batch])\n","\n","    return padded_tokens, attention_mask, labels"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714147966655,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"4BvGc77ARad6"},"outputs":[],"source":["train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n","val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714147966655,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"HEXLr4d2Rcfw","outputId":"07b7d8e6-95df-48af-f958-bf6743986225"},"outputs":[{"name":"stdout","output_type":"stream","text":["tokens: torch.Size([64, 199])\n","atteniton masks: torch.Size([64, 199])\n","labels: torch.Size([64])\n"]}],"source":["for tokens, attention_masks, labels in train_iterator:\n","    print(f'tokens: {tokens.shape}')\n","    print(f'atteniton masks: {attention_masks.shape}')\n","    print(f'labels: {labels.shape}')\n","    break"]},{"cell_type":"markdown","metadata":{"id":"60o3ZPPBSCHu"},"source":["# DistillBERT Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wewChS1rsTsh"},"outputs":[],"source":["import transformers\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","# from torchsummary import summary\n","from tqdm import tqdm\n","\n","# add python path to include src directory\n","import sys\n","sys.path.insert(0, '../src')\n","\n","# standard library imports\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Tuple\n","import math\n","\n","# related third party imports\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizer\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import tqdm\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ze9-urxJSk_"},"outputs":[],"source":["class BertTrainer:\n","    \"\"\" A training and evaluation loop for PyTorch models with a BERT like architecture. \"\"\"\n","\n","\n","    def __init__(\n","        self,\n","        model,\n","        train_dataloader,\n","        eval_dataloader=None,\n","        epochs=1,\n","        lr=5e-04,\n","        output_dir='./',\n","        output_filename='model_state_dict.pt',\n","        save=False,\n","        tabular=False,\n","    ):\n","        \"\"\"\n","        Args:\n","            model: torch.nn.Module: = A PyTorch model with a BERT like architecture,\n","            tokenizer: = A BERT tokenizer for tokenizing text input,\n","            train_dataloader: torch.utils.data.DataLoader =\n","                A dataloader containing the training data with \"text\" and \"label\" keys (optionally a \"tabular\" key),\n","            eval_dataloader: torch.utils.data.DataLoader =\n","                A dataloader containing the evaluation data with \"text\" and \"label\" keys (optionally a \"tabular\" key),\n","            epochs: int = An integer representing the number epochs to train,\n","            lr: float = A float representing the learning rate for the optimizer,\n","            output_dir: str = A string representing the directory path to save the model,\n","            output_filename: string = A string representing the name of the file to save in the output directory,\n","            save: bool = A boolean representing whether or not to save the model,\n","            tabular: bool = A boolean representing whether or not the BERT model is modified to accept tabular data,\n","        \"\"\"\n","\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        self.model = model.to(self.device)\n","        self.train_dataloader = train_dataloader\n","        self.eval_dataloader = eval_dataloader\n","        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.output_dir = output_dir\n","        self.output_filename = output_filename\n","        self.save = save\n","        self.eval_loss = float('inf')  # tracks the lowest loss so as to only save the best model\n","        self.epochs = epochs\n","        self.epoch_best_model = 0  # tracks which epoch the lowest loss is in so as to only save the best model\n","        self.tabular = tabular\n","\n","    def train(self, evaluate=False):\n","        \"\"\" Calls the batch iterator to train and optionally evaluate the model.\"\"\"\n","        for epoch in range(self.epochs):\n","            self.iteration(epoch, self.train_dataloader)\n","            if evaluate and self.eval_dataloader is not None:\n","                self.iteration(epoch, self.eval_dataloader, train=False)\n","\n","    def evaluate(self):\n","        \"\"\" Calls the batch iterator to evaluate the model.\"\"\"\n","        epoch=0\n","        self.iteration(epoch, self.eval_dataloader, train=False)\n","\n","    def iteration(self, epoch, data_loader, train=True):\n","        \"\"\" Iterates through one epoch of training or evaluation\"\"\"\n","\n","        # initialize variables\n","        loss_accumulated = 0.\n","        correct_accumulated = 0\n","        samples_accumulated = 0\n","        preds_all = []\n","        labels_all = []\n","\n","        self.model.train() if train else self.model.eval()\n","\n","        # progress bar\n","        mode = \"train\" if train else \"eval\"\n","        batch_iter = tqdm.tqdm(\n","            enumerate(data_loader),\n","            desc=f\"EP ({mode}) {epoch}\",\n","            total=len(data_loader),\n","            bar_format=\"{l_bar}{r_bar}\"\n","        )\n","\n","        total_comp_time = 0\n","        # iterate through batches of the dataset\n","        for i, batch in batch_iter:\n","            # print(\"Batch: \", batch)\n","            # print(len(batch))\n","            # tokenize data\n","            # batch_t = self.tokenizer(\n","            #     batch[0],\n","            #     padding='max_length',\n","            #     max_length=512,\n","            #     truncation=True,\n","            #     return_tensors='pt',\n","            # )\n","            # batch_t = {key: value.to(self.device) for key, value in batch_t.items()}\n","            # batch_t[\"input_labels\"] = batch[\"label\"].to(self.device)\n","            # batch_t[\"tabular_vectors\"] = batch[\"tabular\"].to(self.device)\n","            # batch = {key: value.to(self.device) for key, value in batch.items()}\n","            # token_types = torch.zeros((len(batch[0][0])), dtype=torch.long).to(self.device)\n","            # forward pass - include tabular data if it is a tabular model\n","\n","            start = time.time()\n","            logits = self.model(\n","                input_ids=batch[0],\n","                attention_mask=batch[1],\n","            ).logits\n","            total_comp_time += time.time() - start\n","\n","            # calculate loss\n","            labels = batch[2].to(self.device)\n","            loss = self.loss_fn(logits, labels)\n","\n","            # compute gradient and and update weights\n","            if train:\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","\n","            # calculate the number of correct predictions\n","            preds = logits.argmax(dim=-1)\n","            correct = preds.eq(labels).sum().item()\n","\n","            # accumulate batch metrics and outputs\n","            loss_accumulated += loss.item()\n","            correct_accumulated += correct\n","            samples_accumulated += len(batch[2])\n","            preds_all.append(preds.detach())\n","            labels_all.append(labels.detach())\n","\n","\n","        average_comp_time = total_comp_time / len(batch_iter)\n","        print(\"Average Comp Time: \", average_comp_time)\n","\n","        # concatenate all batch tensors into one tensor and move to cpu for compatibility with sklearn metrics\n","        preds_all = torch.cat(preds_all, dim=0).cpu()\n","        labels_all = torch.cat(labels_all, dim=0).cpu()\n","\n","        # metrics\n","        accuracy = accuracy_score(labels_all, preds_all)\n","        precision = precision_score(labels_all, preds_all, average='macro')\n","        recall = recall_score(labels_all, preds_all, average='macro')\n","        f1 = f1_score(labels_all, preds_all, average='macro')\n","        avg_loss_epoch = loss_accumulated / len(data_loader)\n","\n","        # print metrics to console\n","        print(\n","            f\"samples={samples_accumulated}, \\\n","            correct={correct_accumulated}, \\\n","            acc={round(accuracy, 4)}, \\\n","            recall={round(recall, 4)}, \\\n","            prec={round(precision,4)}, \\\n","            f1={round(f1, 4)}, \\\n","            loss={round(avg_loss_epoch, 4)}\"\n","        )\n","\n","        # save the model if the evaluation loss is lower than the previous best epoch\n","        if self.save and not train and avg_loss_epoch < self.eval_loss:\n","\n","            # create directory and filepaths\n","            dir_path = Path(self.output_dir)\n","            dir_path.mkdir(parents=True, exist_ok=True)\n","            file_path = dir_path / f\"{self.output_filename}_epoch_{epoch}.pt\"\n","\n","            # delete previous best model from hard drive\n","            if epoch > 0:\n","                file_path_best_model = dir_path / f\"{self.output_filename}_epoch_{self.epoch_best_model}.pt\"\n","                !rm -f $file_path_best_model\n","\n","            # save model\n","            torch.save({\n","                'model_state_dict': self.model.state_dict(),\n","                'optimizer_state_dict': self.optimizer.state_dict()\n","            }, file_path)\n","\n","            # update the new best loss and epoch\n","            self.eval_loss = avg_loss_epoch\n","            self.epoch_best_model = epoch"]},{"cell_type":"markdown","metadata":{"id":"sVvsKB1z8NsD"},"source":["# LoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtnCicQb8Pbv"},"outputs":[],"source":["import math\n","from typing import List, Tuple\n","\n","\n","class LinearLoRA(nn.Module):\n","    \"\"\"\n","    A low-rank adapted linear layer.\n","\n","    Args:\n","        in_dim: int = An integer representing the input dimension of the linear layer\n","        out_dim: int = An integer representing the output dimension of the linear layer\n","        r: int = An integer representing the rank of the low-rank approximated matrices\n","        lora_alpha: int = An integer representing the numerator of the scaling constant alpha / r\n","        lora_dropout: float = A float between 0 and 1 representing the dropout probability\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_dim: int,\n","        out_dim: int,\n","        r: int = 8,\n","        lora_alpha: int = 16,\n","        lora_dropout: float = 0.,\n","    ):\n","        super().__init__()\n","        self.r = r\n","        self.lora_alpha = lora_alpha\n","        self.lora_dropout = nn.Dropout(lora_dropout)\n","\n","        # Check that the rank is at least 1\n","        assert r > 0, \"Variable 'r' is not greater than zero. Choose a rank of 1 or greater.\"\n","\n","        # recreate the linear layer and freeze it (the actual weight values will be copied in outside of this class)\n","        self.pretrained = nn.Linear(in_dim, out_dim, bias=True)\n","        self.pretrained.weight.requires_grad = False\n","\n","        # create the low-rank A matrix and initialize with same method as in Hugging Face PEFT library\n","        self.lora_A = nn.Linear(in_dim, r, bias=False)\n","        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n","\n","        # create the low-rank B matrix and initialize to zero\n","        self.lora_B = nn.Linear(r, out_dim, bias=False)\n","        nn.init.constant_(self.lora_B.weight, 0)\n","\n","        # scaling constant\n","        self.scaling = self.lora_alpha / self.r\n","\n","    def forward(self, x):\n","        pretrained_out = self.pretrained(x)\n","        lora_out = self.lora_dropout(x)\n","        lora_out = self.lora_A(lora_out)\n","        lora_out = self.lora_B(lora_out)\n","        lora_out = lora_out * self.scaling\n","        return pretrained_out + lora_out\n","\n","\n","def freeze_model(model):\n","    \"\"\"Freezes all layers except the LoRa modules and classifier.\"\"\"\n","    for name, param in model.named_parameters():\n","        if \"lora\" not in name and \"classifier\" not in name:\n","            param.requires_grad = False\n","\n","\n","def create_lora(module, r, lora_dropout, lora_alpha):\n","    \"\"\"Converts a linear module to a LoRA linear module.\"\"\"\n","    k, d = module.weight.shape  # pytorch nn.Linear weights are transposed, that is why shape is (k, d) and not (d, k)\n","    lora = LinearLoRA(d, k, r, lora_dropout=lora_dropout, lora_alpha=lora_alpha)\n","    with torch.no_grad():\n","        lora.pretrained.weight.copy_(module.weight)\n","        lora.pretrained.bias.copy_(module.bias)\n","    return lora\n","\n","\n","def add_lora_layers(\n","    model,\n","    module_names: Tuple=(\"query\", \"value\"),\n","    r: int=8,\n","    lora_alpha: float=16,\n","    lora_dropout: float=0.1,\n","    ignore_layers: List[int]=[]\n","):\n","    \"\"\"\n","        Replaces chosen linear modules with LoRA equivalents.\n","\n","        Args:\n","            model: torch.nn.Module = The PyTorch model to be used\n","            module_names: Tuple = A tuple containing the names of the linear layers to replace\n","                Ex. (\"query\") to replace the linear modules with \"query\" in the name --> bert.encoder.layer.0.attention.self.query\n","            r: int =\n","            lora_alpha: int = An integer representing the numerator of the scaling constant alpha / r\n","            lora_dropout: float = A float between 0 and 1 representing the dropout probability\n","            ignore_layers: list = A list with the indices of all BERT layers NOT to add LoRA modules\n","        \"\"\"\n","    module_types: Tuple=(nn.Linear,)\n","\n","    # disable dropout in frozen layers\n","    for module in model.modules():\n","        if isinstance(module, nn.Dropout):\n","            module.p = 0.0\n","    # replace chosen linear modules with lora modules\n","    for name, module in model.named_children():\n","        if isinstance(module, module_types) and name in module_names:\n","            temp_lora = create_lora(module, r=r, lora_dropout=lora_dropout, lora_alpha=lora_alpha)\n","            setattr(model, name, temp_lora)\n","        else:\n","            ignore_layers_str = [str(i) for i in ignore_layers]\n","            if name not in ignore_layers_str:\n","                add_lora_layers(module, module_names, r, lora_dropout, lora_alpha, ignore_layers)\n","\n","\n","def unfreeze_model(model):\n","    \"\"\"Unfreezes all parameters in a model by setting requires_grad to True.\"\"\"\n","    for name, param in model.named_parameters():\n","        param.requires_grad = True\n","\n","\n","def create_linear(module):\n","    \"\"\"Converts a LoRA linear module back to a linear module.\"\"\"\n","    k, d = module.pretrained.weight.shape  # pytorch nn.Linear weights are transposed, that is why variables are k, d and not d, k\n","    linear = nn.Linear(d, k, bias=True)\n","\n","    with torch.no_grad():\n","        linear.weight.copy_(module.pretrained.weight + (module.lora_B.weight @ module.lora_A.weight) * module.scaling)\n","        linear.bias.copy_(module.pretrained.bias)\n","\n","    return linear\n","\n","\n","def merge_lora_layers(model, module_names: Tuple=(\"query\", \"value\"), dropout=0.1):\n","    \"\"\"\n","        Replaces LoRA modules with their original linear equivalents.\n","\n","        Args:\n","            model: torch.nn.Module = The PyTorch model to be used\n","            module_names: Tuple = A tuple containing the names of the LoRA layers to replace\n","                Ex. (\"query\") to replace the LoRA modules with \"query\" in the name --> bert.encoder.layer.0.attention.self.query\n","            r: int =\n","            dropout: float = A float between 0 and 1 representing the dropout probability\n","        \"\"\"\n","    # enable dropout in frozen layers\n","    for module in model.modules():\n","        if isinstance(module, nn.Dropout):\n","            module.p = dropout\n","    # replace chosen linear modules with lora modules\n","    for name, module in model.named_children():\n","        if name in module_names and hasattr(module, \"pretrained\"):\n","            temp_linear = create_linear(module)\n","            setattr(model, name, temp_linear)\n","        else:\n","            merge_lora_layers(module, module_names=module_names, dropout=0.1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":373,"status":"ok","timestamp":1714110545635,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"_ZAnZmt-D3tq","outputId":"d15c4822-1bc5-47e6-eebc-f16447a72ce9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","import time\n","\n","# Load the DistilBERT tokenizer and model\n","# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZuRS2eTN8t2P"},"outputs":[],"source":["add_lora_layers(model, r=1, lora_alpha=2)  # inject the LoRA layers into the model\n","freeze_model(model)  # freeze the non-LoRA parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85,"status":"ok","timestamp":1714110572701,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"gxgmLN4b84js","outputId":"71414f57-a122-4644-a629-fbcbd783af5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 66955010\n","Trainable parameters: 592130\n","Percentage trainable: 0.88%\n"]}],"source":["n_params = 0\n","n_trainable_params = 0\n","\n","# count the number of trainable parameters\n","for n, p in model.named_parameters():\n","    n_params += p.numel()\n","    if p.requires_grad:\n","        n_trainable_params += p.numel()\n","\n","print(f\"Total parameters: {n_params}\")\n","print(f\"Trainable parameters: {n_trainable_params}\")\n","print(f\"Percentage trainable: {round(n_trainable_params / n_params * 100, 2)}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2350660,"status":"error","timestamp":1714119060916,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"ZvCnWqqYKCp3","outputId":"a48078e1-a0a9-429d-8921-4acd19f1a254"},"outputs":[{"name":"stderr","output_type":"stream","text":["EP (train) 0: 100%|| 1417/1417 [15:02<00:00,  1.57it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.6105828668840687\n","samples=45319,             correct=29207,             acc=0.6445,             recall=0.6203,             prec=0.6393,             f1=0.6182,             loss=0.6151\n"]},{"name":"stderr","output_type":"stream","text":["EP (eval) 0: 100%|| 355/355 [03:30<00:00,  1.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5686964437995159\n","samples=11330,             correct=7225,             acc=0.6377,             recall=0.6541,             prec=0.6574,             f1=0.6374,             loss=0.5894\n"]},{"name":"stderr","output_type":"stream","text":["EP (train) 1: 100%|| 1417/1417 [11:31<00:00,  2.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.46286134494385756\n","samples=45319,             correct=31265,             acc=0.6899,             recall=0.6724,             prec=0.6867,             f1=0.6741,             loss=0.5461\n"]},{"name":"stderr","output_type":"stream","text":["EP (eval) 1: 100%|| 355/355 [02:44<00:00,  2.15it/s]"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.4426785838436073\n","samples=11330,             correct=8162,             acc=0.7204,             recall=0.7223,             prec=0.7183,             f1=0.7183,             loss=0.5365\n"]},{"name":"stderr","output_type":"stream","text":["\n","EP (train) 2: 100%|| 1417/1417 [13:08<00:00,  1.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5309502368420889\n","samples=45319,             correct=32205,             acc=0.7106,             recall=0.6974,             prec=0.7066,             f1=0.6995,             loss=0.5162\n"]},{"name":"stderr","output_type":"stream","text":["EP (eval) 2: 100%|| 355/355 [03:11<00:00,  1.86it/s]"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5156473838107687\n","samples=11330,             correct=7726,             acc=0.6819,             recall=0.6361,             prec=0.7663,             f1=0.6132,             loss=0.532\n"]},{"name":"stderr","output_type":"stream","text":["\n","EP (train) 3: 100%|| 1417/1417 [12:52<00:00,  1.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5198688032598502\n","samples=45319,             correct=32445,             acc=0.7159,             recall=0.7033,             prec=0.712,             f1=0.7055,             loss=0.5034\n"]},{"name":"stderr","output_type":"stream","text":["EP (eval) 3: 100%|| 355/355 [03:08<00:00,  1.88it/s]"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5084647554746816\n","samples=11330,             correct=8154,             acc=0.7197,             recall=0.69,             prec=0.7438,             f1=0.6897,             loss=0.5065\n"]},{"name":"stderr","output_type":"stream","text":["\n","EP (train) 4: 100%|| 1417/1417 [13:17<00:00,  1.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5376488970633367\n","samples=45319,             correct=32710,             acc=0.7218,             recall=0.7096,             prec=0.718,             f1=0.7118,             loss=0.4952\n"]},{"name":"stderr","output_type":"stream","text":["EP (eval) 4: 100%|| 355/355 [02:59<00:00,  1.97it/s]"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.4842192226732281\n","samples=11330,             correct=8417,             acc=0.7429,             recall=0.7379,             prec=0.738,             f1=0.7379,             loss=0.4807\n"]},{"name":"stderr","output_type":"stream","text":["\n","EP (train) 5: 100%|| 1417/1417 [13:02<00:00,  1.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5272237130597854\n","samples=45319,             correct=32872,             acc=0.7253,             recall=0.7143,             prec=0.7212,             f1=0.7163,             loss=0.4881\n"]},{"name":"stderr","output_type":"stream","text":["EP (eval) 5: 100%|| 355/355 [03:10<00:00,  1.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5135153320473684\n","samples=11330,             correct=8226,             acc=0.726,             recall=0.7041,             prec=0.7333,             f1=0.7067,             loss=0.4869\n"]},{"name":"stderr","output_type":"stream","text":["EP (train) 6: 100%|| 1417/1417 [13:13<00:00,  1.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5349347079902277\n","samples=45319,             correct=33119,             acc=0.7308,             recall=0.721,             prec=0.7265,             f1=0.7228,             loss=0.4799\n"]},{"name":"stderr","output_type":"stream","text":["EP (eval) 6: 100%|| 355/355 [03:00<00:00,  1.96it/s]"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.48659731233623665\n","samples=11330,             correct=8319,             acc=0.7342,             recall=0.7224,             prec=0.7309,             f1=0.7248,             loss=0.4688\n"]},{"name":"stderr","output_type":"stream","text":["\n","EP (train) 7: 100%|| 1417/1417 [12:32<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.5058631094392119\n","samples=45319,             correct=33304,             acc=0.7349,             recall=0.7248,             prec=0.7309,             f1=0.7267,             loss=0.4766\n"]},{"name":"stderr","output_type":"stream","text":["EP (eval) 7: 100%|| 355/355 [03:02<00:00,  1.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Average Comp Time:  0.4919233288563473\n","samples=11330,             correct=7842,             acc=0.6921,             recall=0.6516,             prec=0.7486,             f1=0.6385,             loss=0.5484\n"]},{"name":"stderr","output_type":"stream","text":["EP (train) 8:  85%|| 1211/1417 [11:47<02:00,  1.71it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-cd80eedb18db>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer_distill_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Define a function to preprocess the input text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-e71fa53c7541>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, evaluate)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;34m\"\"\" Calls the batch iterator to train and optionally evaluate the model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-e71fa53c7541>\u001b[0m in \u001b[0;36miteration\u001b[0;34m(self, epoch, data_loader, train)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             logits = self.model(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    815\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 )\n\u001b[1;32m    574\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    576\u001b[0m                     \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Feed Forward Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0mffn_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msa_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_chunking_to_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer_distill_bert = BertTrainer(\n","    model,\n","    lr=0.001,\n","    epochs=10,\n","    train_dataloader=train_iterator,\n","    eval_dataloader=val_iterator,\n","    output_dir='../models/distill_bert',\n","    output_filename='distill_bert',\n","    save=True,\n",")\n","\n","trainer_distill_bert.train(evaluate=True)\n","\n","# Define a function to preprocess the input text\n","# def preprocess(text):\n","#     encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","#     return encoding\n","\n","# Example usage\n","# text = \"This movie was absolutely fantastic! I loved every minute of it.\"\n","\n","# # Preprocess the input text\n","# encoding = preprocess(text)\n","\n","# Pass the input through the DistilBERT model\n","\n","\n","\n","\n","# output = model(**encoding)\n","\n","# # Get the predicted class label\n","# predicted_label = output.logits.argmax(-1).item()\n","\n","# print(f\"Predicted class label: {predicted_label}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"elapsed":5166,"status":"ok","timestamp":1714147971819,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"Q4z73W6iCNWa","outputId":"c22a3e4c-897a-4366-dbdd-0f8b3c651ad4"},"outputs":[],"source":["import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","import time\n","\n","# Load the DistilBERT tokenizer and model\n","# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1714147972107,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"HIFtQqCeGxSc","outputId":"e9a3ebfe-dd11-41da-b830-b356fc8b3a15"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["checkpoint = torch.load('distill_bert_epoch_6.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1714147972107,"user":{"displayName":"sk","userId":"05142611603552977395"},"user_tz":240},"id":"oc3cGdhVGyyE","outputId":"2f533cda-c8d4-4f12-cdc4-1ae4a3355c55"},"outputs":[{"data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBZgWprGHDkp","outputId":"82ff02b7-74fb-4625-b310-5a84e1ad3b1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["23.985334634780884\n"]}],"source":["total_comp_time = 0\n","count = 0\n","for tokens, attention_masks, labels in val_iterator:\n","    # print(f'tokens: {tokens.shape}')\n","    # print(f'atteniton masks: {attention_masks.shape}')\n","    # print(f'labels: {labels.shape}')\n","    start = time.time()\n","    output = model(tokens)\n","    total_comp_time += time.time() - start\n","    print(total_comp_time)\n","    count += 1\n","print(count)\n","print(len(val_iterator))\n","print(\"Average Time: \", total_comp_time/len(val_iterator))"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"28c22b9fbcc84309a97c17dd065d99fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37e4967bb0b246c49453e492a63908f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3860c30f24cd4cd7ae250be814eb6fd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b08120858a946ee8af7cded3b95b22f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d40a4a4c731240b4913de60eb453309f","IPY_MODEL_bf2fece49a3849448e7462b76bc00476","IPY_MODEL_ef14c7bc92e04507a81dbb57c926c35e"],"layout":"IPY_MODEL_28c22b9fbcc84309a97c17dd065d99fd"}},"4ee3db2aafea4fe2b9f98367aace4bd1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"500b1bd6359545e38c43566a0337ff81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61e054fa74514c6ba6dc8f0451108df6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"626160068b5240369d6fd30c4e3426a3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63e518e21fea428eb542488a583f9925":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69b6586a488148209ad5f7dce48179d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"765e2a1d206d4231a1782cc0c9622156":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3860c30f24cd4cd7ae250be814eb6fd7","placeholder":"","style":"IPY_MODEL_69b6586a488148209ad5f7dce48179d8","value":"config.json:100%"}},"7d65da1c43b54ddbbdc5b96a6b902eaa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da5ef46802f4e53889130699237cee9","placeholder":"","style":"IPY_MODEL_f121ff29b3124cf494a90be5ca98383e","value":"483/483[00:00&lt;00:00,26.9kB/s]"}},"7da5ef46802f4e53889130699237cee9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96e1968536f9462ebb28633b51953949":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b61c1dd73cc49e3aa35d1b124472371":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_765e2a1d206d4231a1782cc0c9622156","IPY_MODEL_b7a41b77d4ae4510b90641b75b4c82ce","IPY_MODEL_7d65da1c43b54ddbbdc5b96a6b902eaa"],"layout":"IPY_MODEL_4ee3db2aafea4fe2b9f98367aace4bd1"}},"9d2c183fc72342bca0827766971c3678":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7a41b77d4ae4510b90641b75b4c82ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_37e4967bb0b246c49453e492a63908f9","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_500b1bd6359545e38c43566a0337ff81","value":483}},"bf2fece49a3849448e7462b76bc00476":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63e518e21fea428eb542488a583f9925","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea73388ca9a847d7a2bfba612130d0ef","value":267954768}},"d40a4a4c731240b4913de60eb453309f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2c183fc72342bca0827766971c3678","placeholder":"","style":"IPY_MODEL_61e054fa74514c6ba6dc8f0451108df6","value":"model.safetensors:100%"}},"ea73388ca9a847d7a2bfba612130d0ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef14c7bc92e04507a81dbb57c926c35e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_626160068b5240369d6fd30c4e3426a3","placeholder":"","style":"IPY_MODEL_96e1968536f9462ebb28633b51953949","value":"268M/268M[00:01&lt;00:00,182MB/s]"}},"f121ff29b3124cf494a90be5ca98383e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
