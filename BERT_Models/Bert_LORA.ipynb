{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XFbiyvdlGN9u"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset, RandomSampler\n","\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import random"]},{"cell_type":"markdown","metadata":{"id":"tYR1S8cqKNLm"},"source":["# CSIC and Vocab Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S65-90NlKMll"},"outputs":[],"source":["import os\n","from typing import List\n","from collections import Counter\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import ByteLevel\n","\n","from urllib import parse\n","\n","class CSICDataset(Dataset):\n","    def __init__(self, df: pd.DataFrame, vocab=None, vocab_size=1000, min_frequency=1, special_tokens=[\"[UNK]\",\"[CLS]\",\"[PAD]\"], tokenization_algorithm=\"bpe\"):\n","        self.df = df\n","\n","        # Export text content to csv for learning tokenization; apply BPE\n","        path = os.path.join('.', 'tokenization_input')\n","\n","        self.df.to_csv(path_or_buf=path, columns=['content_for_tokenization'], index=False, header=False)\n","\n","        if vocab == None:\n","            vocab = Vocab(vocab_size=vocab_size, min_frequency=min_frequency,\n","                           special_tokens=special_tokens,\n","                           tokenization_algorithm=tokenization_algorithm)\n","            vocab.build(corpus_files=[path])\n","\n","        self.vocab = vocab\n","\n","        self.encode_df()\n","\n","\n","    @staticmethod\n","    def process_df(df: pd.DataFrame):\n","        # Pre-process data by dropping rows without POST-Data or GET-Query\n","        get_mask, post_mask = df['GET-Query'].notna(), df['POST-Data'].notna()\n","\n","        df.loc[get_mask,\"content_for_tokenization\"] = df.loc[get_mask,\"GET-Query\"]\n","        df.loc[post_mask,\"content_for_tokenization\"] = df.loc[post_mask,\"POST-Data\"]\n","\n","        df = df[get_mask | post_mask]\n","        df = df.drop(columns=[\"GET-Query\",\"POST-Data\", \"Accept-Charset\", \"Accept-Language\", \"Accept\", \"Cache-control\", \"Pragma\", \"Content-Type\", \"Host-Header\", \"Connection\"])\n","\n","        return df\n","\n","    def encode_df(self):\n","        # Tokenize the GET-Query and POST-Data columns according to the subword vocabulary learned from BPE\n","        self.df[\"tokenized_ids\"] = self.df[\"content_for_tokenization\"].apply(lambda x: self.vocab.words2indices(x))\n","        self.df[\"tokenized\"] = self.df[\"content_for_tokenization\"].apply(lambda x: self.vocab.tokenize(x))\n","        self.df = self.df.drop(columns=[\"content_for_tokenization\"])\n","\n","        self.class_encoder, self.method_encoder = LabelEncoder(), LabelEncoder()\n","        self.df['Class'], self.df['Method'] = self.class_encoder.fit_transform(self.df['Class']), self.class_encoder.fit_transform(self.df['Method'])\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        features = self.df.iloc[index].drop(['Class', 'User-Agent'])\n","        label = self.df.iloc[index]['Class']\n","\n","        return features, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Rj6eSuvKWg_"},"outputs":[],"source":["class Vocab(object):\n","    def __init__(self, vocab_size=0, min_frequency=0, special_tokens: List[str]=[], unk_token=\"[UNK]\", pad_token=\"[PAD]\", tokenizer=None, tokenization_algorithm=\"bpe\"):\n","        if tokenizer:\n","            self.tokenizer = tokenizer\n","\n","            self.word2id = tokenizer.get_vocab()\n","            self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","            self.unk_id = self.word2id[unk_token]\n","\n","        else:\n","            assert vocab_size > 0\n","            assert min_frequency > 0\n","\n","            self.vocab_size = vocab_size\n","            self.min_frequency = min_frequency\n","            self.special_tokens = special_tokens\n","            self.unk_token = unk_token\n","            self.pad_token = pad_token\n","            self.tokenization_algorithm = tokenization_algorithm\n","\n","    def build(self, corpus_files: List[str]):\n","        if self.tokenization_algorithm == 'bpe':\n","            tokenizer = Tokenizer(BPE(unk_token=self.unk_token))\n","            trainer = BpeTrainer(vocab_size=self.vocab_size, min_frequency=self.min_frequency, special_tokens=self.special_tokens)\n","            tokenizer.pre_tokenizer = ByteLevel()\n","\n","            tokenizer.train(corpus_files, trainer)\n","\n","            self.tokenizer = tokenizer\n","            self.word2id = tokenizer.get_vocab()\n","            self.id2word = {v: k for k, v in self.word2id.items()}\n","\n","        elif self.tokenization_algorithm == 'vocab_map':\n","            self.word2id, self.id2word = dict(), dict()\n","            curr_id, self.min_frequency = 1,1\n","            counter = Counter()\n","\n","            def add_to_vocab(token: str, ignore_cutoff=False):\n","                nonlocal curr_id\n","                if (ignore_cutoff or counter[token] >= self.min_frequency) and token not in self.word2id:\n","                    self.word2id[token] = curr_id\n","                    self.id2word[curr_id] = token\n","\n","                    curr_id += 1\n","\n","            for file_path in corpus_files:\n","                with open(file_path, 'r') as file:\n","                    for line in file:\n","                        tokens = Vocab.parse_req_body_or_params(line)\n","                        counter.update(tokens)\n","\n","            unwanted_tokens = [' ','']\n","            for token in unwanted_tokens:\n","                if token in counter:\n","                    del counter[token]\n","\n","            for token in self.special_tokens:\n","                add_to_vocab(token, ignore_cutoff=True)\n","\n","            for token in set(counter.elements()):\n","                add_to_vocab(token)\n","\n","        else:\n","            raise TypeError(\"Unsupported tokenization algorithm detected\")\n","\n","        self.unk_id = self.word2id[self.unk_token]\n","        self.pad_id = self.word2id[self.pad_token]\n","\n","    def __getitem__(self, word):\n","        return self.word2id.get(word, self.unk_id)\n","\n","    def __contains__(self, word):\n","        return word in self.word2id\n","\n","    def __setitem__(self, key, value):\n","        raise ValueError('vocabulary is readonly')\n","\n","    def __len__(self):\n","        return len(self.word2id)\n","\n","    def __repr__(self):\n","        return 'Vocabulary[size=%d]' % len(self)\n","\n","    def id2word(self, wid):\n","        return self.id2word[wid]\n","\n","    def save(self, file_path):\n","        self.tokenizer.save(path=file_path)\n","\n","    def words2indices(self, content):\n","        if self.tokenization_algorithm == 'bpe':\n","            if type(content) == list:\n","                return [self.tokenizer.encode(row).ids for row in content]\n","            else:\n","                return self.tokenizer.encode(content).ids\n","\n","        elif self.tokenization_algorithm == 'vocab_map':\n","            if type(content) == list:\n","                return [[self[token] for token in Vocab.parse_req_body_or_params(line)] for line in content]\n","            else:\n","                return [self[token] for token in Vocab.parse_req_body_or_params(content)]\n","        else:\n","            raise TypeError(\"Unsupported tokenization algorithm detected\")\n","\n","    def tokenize(self, content):\n","        if self.tokenization_algorithm == 'bpe':\n","            if type(content) == list:\n","                return [self.tokenizer.encode(row).tokens for row in content]\n","            else:\n","                return self.tokenizer.encode(content).tokens\n","\n","        elif self.tokenization_algorithm == 'vocab_map':\n","            if type(content) == list:\n","                return [[token if self.__contains__(token) else self.unk_token for token in Vocab.parse_req_body_or_params(line)] for line in content]\n","            else:\n","                return [token if self.__contains__(token) else self.unk_token for token in Vocab.parse_req_body_or_params(content)]\n","\n","        else:\n","            raise TypeError(\"Unsupported tokenization algorithm detected\")\n","\n","    @staticmethod\n","    def parse_req_body_or_params(line: str):\n","        parsed_line = parse.parse_qs(parse.unquote_plus(string=line))\n","\n","        tokens = []\n","        for k, v in parsed_line.items():\n","            tokens.append(k)\n","            tokens.extend(v)\n","\n","        return tokens\n","\n","    @staticmethod\n","    def load(file_path: str):\n","        return Vocab(tokenizer=Tokenizer.from_file(file_path))"]},{"cell_type":"markdown","metadata":{"id":"LLCCxrOzKdaN"},"source":["# Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bi76zAr_JpbV"},"outputs":[],"source":["# Defining global constants\n","RANDOM_SEED = 42\n","BATCH_SIZE = 32\n","\n","torch.manual_seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714070358402,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"FiQJ39fWKlBI","outputId":"0312385f-c744-41ad-80c6-4aed74e5fa4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device of execution -  cuda\n"]}],"source":["# This is how we select a GPU if it's available on your computer or in the Colab environment.\n","print('Device of execution - ', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":4,"status":"error","timestamp":1714070359510,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"_fsd7PRFKpPT","outputId":"090025f6-27b6-4df4-8d9f-9a1d63376424"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./dataset.csv')\n","df = CSICDataset.process_df(df)\n","\n","# # The following two lines are used to load the indices of the training and validation sets\n","# train_indices = np.load('./dataset/train_indices.npy')\n","# val_indices = np.load('./dataset/val_indices.npy')\n","\n","# # The following two lines are used to select the training and validation sets from the dataframe based on the indices loaded above\n","# train_data = df.loc[train_indices].reset_index(drop=True)\n","# val_data = df.loc[val_indices].reset_index(drop=True)\n","\n","train_data, val_data = train_test_split(df, test_size=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8E1liYAQ9yL"},"outputs":[],"source":["train_dataset = CSICDataset(df=train_data, vocab_size=5000, min_frequency=1, tokenization_algorithm='bpe')\n","train_vocab = train_dataset.vocab\n","\n","val_dataset = CSICDataset(df=val_data, vocab=train_vocab)\n","\n","train_sampler = RandomSampler(train_dataset)\n","val_sampler   = RandomSampler(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1713993302476,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"FbUOjJ8aROv4","outputId":"21d5e0ae-9604-47be-e7cf-73ce9ad25262"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training and Validation dataset sizes match!\n"]}],"source":["# Check Dataset Lengths\n","assert len(train_dataset) == 45319, \"Training Dataset is of incorrect size\"\n","assert len(val_dataset) == 11330, \"Validation Dataset is of incorrect size\"\n","\n","print('Training and Validation dataset sizes match!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ihg6xpQtRW5S"},"outputs":[],"source":["PADDING_VALUE = train_vocab.pad_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZnYLvVVRYgx"},"outputs":[],"source":["def collate_fn(batch, padding_value=PADDING_VALUE):\n","    # Batch is of the form List[Tuple(Features(tokenized_ids,...), Labels)]\n","    sequences = [torch.tensor(sample[0]['tokenized_ids'], dtype=torch.long, device=device) for sample in batch]\n","    padded_tokens = torch.nn.utils.rnn.pad_sequence(sequences=sequences,batch_first=True, padding_value=padding_value)\n","    attention_mask = (padded_tokens != padding_value).float()\n","    labels = torch.tensor([sample[1] for sample in batch])\n","\n","    return padded_tokens, attention_mask, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BvGc77ARad6"},"outputs":[],"source":["train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n","val_iterator   = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":480,"status":"ok","timestamp":1713993302929,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"HEXLr4d2Rcfw","outputId":"557fc01c-7576-42dc-b7e7-cc8d4a63e7e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["tokens: torch.Size([32, 172])\n","atteniton masks: torch.Size([32, 172])\n","labels: torch.Size([32])\n"]}],"source":["for tokens, attention_masks, labels in train_iterator:\n","    print(f'tokens: {tokens.shape}')\n","    print(f'atteniton masks: {attention_masks.shape}')\n","    print(f'labels: {labels.shape}')\n","    break"]},{"cell_type":"markdown","metadata":{"id":"60o3ZPPBSCHu"},"source":["# BERT Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5469,"status":"ok","timestamp":1714070368058,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"Pxzbpxr7SKnh","outputId":"feb1ccee-a0e8-48e4-b7db-38779c03596c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qcA-qxCRdj0"},"outputs":[],"source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTX4l1iYTLvj"},"outputs":[],"source":["import transformers\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchsummary import summary\n","from tqdm import tqdm\n","\n","# add python path to include src directory\n","import sys\n","sys.path.insert(0, '../src')\n","\n","# standard library imports\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Tuple\n","import math\n","\n","# related third party imports\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from torch.utils.data import DataLoader\n","from transformers import BertTokenizer\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import tqdm\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIalsn0YXNll"},"outputs":[],"source":["class BertTrainer:\n","    \"\"\" A training and evaluation loop for PyTorch models with a BERT like architecture. \"\"\"\n","\n","\n","    def __init__(\n","        self,\n","        model,\n","        tokenizer,\n","        train_dataloader,\n","        eval_dataloader=None,\n","        epochs=1,\n","        lr=5e-04,\n","        output_dir='./',\n","        output_filename='model_state_dict.pt',\n","        save=False,\n","        tabular=False,\n","    ):\n","        \"\"\"\n","        Args:\n","            model: torch.nn.Module: = A PyTorch model with a BERT like architecture,\n","            tokenizer: = A BERT tokenizer for tokenizing text input,\n","            train_dataloader: torch.utils.data.DataLoader =\n","                A dataloader containing the training data with \"text\" and \"label\" keys (optionally a \"tabular\" key),\n","            eval_dataloader: torch.utils.data.DataLoader =\n","                A dataloader containing the evaluation data with \"text\" and \"label\" keys (optionally a \"tabular\" key),\n","            epochs: int = An integer representing the number epochs to train,\n","            lr: float = A float representing the learning rate for the optimizer,\n","            output_dir: str = A string representing the directory path to save the model,\n","            output_filename: string = A string representing the name of the file to save in the output directory,\n","            save: bool = A boolean representing whether or not to save the model,\n","            tabular: bool = A boolean representing whether or not the BERT model is modified to accept tabular data,\n","        \"\"\"\n","\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        self.model = model.to(self.device)\n","        self.tokenizer = tokenizer\n","        self.train_dataloader = train_dataloader\n","        self.eval_dataloader = eval_dataloader\n","        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.output_dir = output_dir\n","        self.output_filename = output_filename\n","        self.save = save\n","        self.eval_loss = float('inf')  # tracks the lowest loss so as to only save the best model\n","        self.epochs = epochs\n","        self.epoch_best_model = 0  # tracks which epoch the lowest loss is in so as to only save the best model\n","        self.tabular = tabular\n","\n","    def train(self, evaluate=False):\n","        \"\"\" Calls the batch iterator to train and optionally evaluate the model.\"\"\"\n","        for epoch in range(self.epochs):\n","            self.iteration(epoch, self.train_dataloader)\n","            if evaluate and self.eval_dataloader is not None:\n","                self.iteration(epoch, self.eval_dataloader, train=False)\n","\n","    def evaluate(self):\n","        \"\"\" Calls the batch iterator to evaluate the model.\"\"\"\n","        epoch=0\n","        self.iteration(epoch, self.eval_dataloader, train=False)\n","\n","    def iteration(self, epoch, data_loader, train=True):\n","        \"\"\" Iterates through one epoch of training or evaluation\"\"\"\n","\n","        # initialize variables\n","        loss_accumulated = 0.\n","        correct_accumulated = 0\n","        samples_accumulated = 0\n","        preds_all = []\n","        labels_all = []\n","\n","        self.model.train() if train else self.model.eval()\n","\n","        # progress bar\n","        mode = \"train\" if train else \"eval\"\n","        batch_iter = tqdm.tqdm(\n","            enumerate(data_loader),\n","            desc=f\"EP ({mode}) {epoch}\",\n","            total=len(data_loader),\n","            bar_format=\"{l_bar}{r_bar}\"\n","        )\n","\n","        total_comp_time = 0\n","        # iterate through batches of the dataset\n","        for i, batch in batch_iter:\n","            # print(\"Batch: \", batch)\n","            # print(len(batch))\n","            # tokenize data\n","            # batch_t = self.tokenizer(\n","            #     batch[0],\n","            #     padding='max_length',\n","            #     max_length=512,\n","            #     truncation=True,\n","            #     return_tensors='pt',\n","            # )\n","            # batch_t = {key: value.to(self.device) for key, value in batch_t.items()}\n","            # batch_t[\"input_labels\"] = batch[\"label\"].to(self.device)\n","            # batch_t[\"tabular_vectors\"] = batch[\"tabular\"].to(self.device)\n","            # batch = {key: value.to(self.device) for key, value in batch.items()}\n","            token_types = torch.zeros((len(batch[0][0])), dtype=torch.long).to(self.device)\n","            # forward pass - include tabular data if it is a tabular model\n","            if self.tabular:\n","                logits = self.model(\n","                    input_ids=batch_t[\"input_ids\"],\n","                    token_type_ids=batch_t[\"token_type_ids\"],\n","                    attention_mask=batch_t[\"attention_mask\"],\n","                    tabular_vectors=batch_t[\"tabular_vectors\"],\n","                )\n","\n","            else:\n","                start = time.time()\n","                logits = self.model(\n","                    input_ids=batch[0],\n","                    token_type_ids=token_types,\n","                    attention_mask=batch[1],\n","                ).logits\n","                total_comp_time += time.time() - start\n","\n","            # calculate loss\n","            labels = batch[2].to(self.device)\n","            loss = self.loss_fn(logits, labels)\n","\n","            # compute gradient and and update weights\n","            if train:\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","\n","            # calculate the number of correct predictions\n","            preds = logits.argmax(dim=-1)\n","            correct = preds.eq(labels).sum().item()\n","\n","            # accumulate batch metrics and outputs\n","            loss_accumulated += loss.item()\n","            correct_accumulated += correct\n","            samples_accumulated += len(batch[2])\n","            preds_all.append(preds.detach())\n","            labels_all.append(labels.detach())\n","\n","\n","        average_comp_time = total_comp_time / len(batch_iter)\n","        print(\"Average Comp Time: \", average_comp_time)\n","\n","        # concatenate all batch tensors into one tensor and move to cpu for compatibility with sklearn metrics\n","        preds_all = torch.cat(preds_all, dim=0).cpu()\n","        labels_all = torch.cat(labels_all, dim=0).cpu()\n","\n","        # metrics\n","        accuracy = accuracy_score(labels_all, preds_all)\n","        precision = precision_score(labels_all, preds_all, average='macro')\n","        recall = recall_score(labels_all, preds_all, average='macro')\n","        f1 = f1_score(labels_all, preds_all, average='macro')\n","        avg_loss_epoch = loss_accumulated / len(data_loader)\n","\n","        # print metrics to console\n","        print(\n","            f\"samples={samples_accumulated}, \\\n","            correct={correct_accumulated}, \\\n","            acc={round(accuracy, 4)}, \\\n","            recall={round(recall, 4)}, \\\n","            prec={round(precision,4)}, \\\n","            f1={round(f1, 4)}, \\\n","            loss={round(avg_loss_epoch, 4)}\"\n","        )\n","\n","        # save the model if the evaluation loss is lower than the previous best epoch\n","        if self.save and not train and avg_loss_epoch < self.eval_loss:\n","\n","            # create directory and filepaths\n","            dir_path = Path(self.output_dir)\n","            dir_path.mkdir(parents=True, exist_ok=True)\n","            file_path = dir_path / f\"{self.output_filename}_epoch_{epoch}.pt\"\n","\n","            # delete previous best model from hard drive\n","            if epoch > 0:\n","                file_path_best_model = dir_path / f\"{self.output_filename}_epoch_{self.epoch_best_model}.pt\"\n","                !rm -f $file_path_best_model\n","\n","            # save model\n","            torch.save({\n","                'model_state_dict': self.model.state_dict(),\n","                'optimizer_state_dict': self.optimizer.state_dict()\n","            }, file_path)\n","\n","            # update the new best loss and epoch\n","            self.eval_loss = avg_loss_epoch\n","            self.epoch_best_model = epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316,"referenced_widgets":["b72d61e9af1a40968d25b40a1ce1b4f1","adda16a85416431ea34948a3675c3458","ed44ba274a3441c094f1fb120f9e730e","e332c33d5cb8423d8b08b7dd51f79f9f","b41b1a04798c4bc7b85c485d297fd7ed","71275f1cc8894952af1fa9c502bb241c","8228cb05b61349fba8e263bd46125b46","5046559880ed427b92dd39d18a2dd78b","d9446a77daff4c138986acd027bc680c","2f284ca64fa0491ea26e2e53dacfbd17","147a20d590ab4ddebea3284ecc977cf0","d09164bce11c4bb0b267c6e5b5adfba6","3a5cc069e13b4d289e42299aa65f728e","e931a1c4773d41d3aa8d42807c639969","7b89194772174c81b8266fbdb5d25b00","c526e7c0a5c2492b986b9f63dde601de","529c27136cef407cb14f23c2844a497e","449ce7f6f7624f9f8624dac9987e153c","3ae3bba4c1a5495fa081a52bf25db3b3","ed063d78c3cd47f7a0c3594b9a4cda70","4d2cf59141474db0bbcc5bd873d0e56e","3b2ed46290404a86af3d48eddd7c385b","7a3ff827a22840aa86db87e6f6a8daf6","235d095e834d4e8da6aba14e3bdc0cce","af3f762a199841a3a8b971c1e7cefad9","696c2b72bdd643f59cdf20f4bcafb5fc","9a37b23a152b43408ead3c5afd28a8a5","50ba40d5e26d4d5591bf2403e01c1769","e8523052537c4162ae8985fbc3c94713","59a6c7d4c7674408a92828e7453223b3","bf4c05beb60140359462d2f4707aaf98","29965c2ea66c483ab2faa6fdb8929567","52a7c9f4250b40879a43b063015dde20","17d2127c8fb54cbc95d7ab54032f3ae5","4020a8bc1c4642f988819c984800f6c8","4924d438bba44b18947194eb835192da","b2d007d8190149308478b4b4ee80014f","00f5c793f03d44afb7821246a925de3b","89b9f8eb8b4742b2b18c050588ccb4a1","bbe6ae8cdb5946c99f01df36266d2208","d7410464a4a74296a2618025c26106ff","dfb6c7cfd2b94416bcc8e0d9c388380e","a9f7feecf09a4a5d972a622eaca64efa","759eed341c9b4779a787bfb24f2216b3","942ac2e220ae4fe4986c1d8dbac0c4d8","a3cfe886b0b540328f20d692480e18ff","04b030899804411e8553593be898d42c","9ad4645a0f554415929d72273dbf418b","4c62e005028e4533a165be4d6850b968","6755e84ff3554f8193d8eaec816d6381","4e08bf177d1a42daa8fb759408e1fd7f","bab052357df7406a9a44147cfe51b336","e09470c15b264147bf3149261e66d920","4e2a903968f9482f835f132fcb12a5c2","a1ab21cf88aa49e38380735b2003057a"]},"executionInfo":{"elapsed":6428,"status":"ok","timestamp":1714070393356,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"A0LuSHXKSILV","outputId":"8267dd4a-b8cc-4a9f-bd72-550a24331740"},"outputs":[],"source":["# class BERT(nn.Module):\n","#     def __init__(self):\n","#         super(BERT, self).__init__()\n","#         self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n","#         self.out = nn.Linear(768, 1)\n","\n","#     def forward(self,ids,mask,token_type_ids):\n","#         _,o2= self.bert_model(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n","\n","#         out= self.out(o2)\n","\n","#         return out\n","\n","# model=BERT()\n","\n","# loss_fn = nn.BCEWithLogitsLoss()\n","\n","# #Initialize Optimizer\n","# optimizer= optim.Adam(model.parameters(),lr= 0.0001)\n","\n","\n","# load tokenizer and pretrained model\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Load the latest version of the BERT tokenizer\n","tokenizer_base = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Load the latest version of the BERT model\n","bert_base = AutoModelForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels=2  # number of classes\n",")"]},{"cell_type":"markdown","metadata":{"id":"V8xmDOBoT0iH"},"source":["# LORA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DS-mKskoWoXK"},"outputs":[],"source":["import math\n","from typing import List, Tuple\n","\n","\n","class LinearLoRA(nn.Module):\n","    \"\"\"\n","    A low-rank adapted linear layer.\n","\n","    Args:\n","        in_dim: int = An integer representing the input dimension of the linear layer\n","        out_dim: int = An integer representing the output dimension of the linear layer\n","        r: int = An integer representing the rank of the low-rank approximated matrices\n","        lora_alpha: int = An integer representing the numerator of the scaling constant alpha / r\n","        lora_dropout: float = A float between 0 and 1 representing the dropout probability\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_dim: int,\n","        out_dim: int,\n","        r: int = 8,\n","        lora_alpha: int = 16,\n","        lora_dropout: float = 0.,\n","    ):\n","        super().__init__()\n","        self.r = r\n","        self.lora_alpha = lora_alpha\n","        self.lora_dropout = nn.Dropout(lora_dropout)\n","\n","        # Check that the rank is at least 1\n","        assert r > 0, \"Variable 'r' is not greater than zero. Choose a rank of 1 or greater.\"\n","\n","        # recreate the linear layer and freeze it (the actual weight values will be copied in outside of this class)\n","        self.pretrained = nn.Linear(in_dim, out_dim, bias=True)\n","        self.pretrained.weight.requires_grad = False\n","\n","        # create the low-rank A matrix and initialize with same method as in Hugging Face PEFT library\n","        self.lora_A = nn.Linear(in_dim, r, bias=False)\n","        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n","\n","        # create the low-rank B matrix and initialize to zero\n","        self.lora_B = nn.Linear(r, out_dim, bias=False)\n","        nn.init.constant_(self.lora_B.weight, 0)\n","\n","        # scaling constant\n","        self.scaling = self.lora_alpha / self.r\n","\n","    def forward(self, x):\n","        pretrained_out = self.pretrained(x)\n","        lora_out = self.lora_dropout(x)\n","        lora_out = self.lora_A(lora_out)\n","        lora_out = self.lora_B(lora_out)\n","        lora_out = lora_out * self.scaling\n","        return pretrained_out + lora_out\n","\n","\n","def freeze_model(model):\n","    \"\"\"Freezes all layers except the LoRa modules and classifier.\"\"\"\n","    for name, param in model.named_parameters():\n","        if \"lora\" not in name and \"classifier\" not in name:\n","            param.requires_grad = False\n","\n","\n","def create_lora(module, r, lora_dropout, lora_alpha):\n","    \"\"\"Converts a linear module to a LoRA linear module.\"\"\"\n","    k, d = module.weight.shape  # pytorch nn.Linear weights are transposed, that is why shape is (k, d) and not (d, k)\n","    lora = LinearLoRA(d, k, r, lora_dropout=lora_dropout, lora_alpha=lora_alpha)\n","    with torch.no_grad():\n","        lora.pretrained.weight.copy_(module.weight)\n","        lora.pretrained.bias.copy_(module.bias)\n","    return lora\n","\n","\n","def add_lora_layers(\n","    model,\n","    module_names: Tuple=(\"query\", \"value\"),\n","    r: int=8,\n","    lora_alpha: float=16,\n","    lora_dropout: float=0.1,\n","    ignore_layers: List[int]=[]\n","):\n","    \"\"\"\n","        Replaces chosen linear modules with LoRA equivalents.\n","\n","        Args:\n","            model: torch.nn.Module = The PyTorch model to be used\n","            module_names: Tuple = A tuple containing the names of the linear layers to replace\n","                Ex. (\"query\") to replace the linear modules with \"query\" in the name --> bert.encoder.layer.0.attention.self.query\n","            r: int =\n","            lora_alpha: int = An integer representing the numerator of the scaling constant alpha / r\n","            lora_dropout: float = A float between 0 and 1 representing the dropout probability\n","            ignore_layers: list = A list with the indices of all BERT layers NOT to add LoRA modules\n","        \"\"\"\n","    module_types: Tuple=(nn.Linear,)\n","\n","    # disable dropout in frozen layers\n","    for module in model.modules():\n","        if isinstance(module, nn.Dropout):\n","            module.p = 0.0\n","    # replace chosen linear modules with lora modules\n","    for name, module in model.named_children():\n","        print(\"Name: \", name)\n","        if isinstance(module, module_types) and name in module_names:\n","            temp_lora = create_lora(module, r=r, lora_dropout=lora_dropout, lora_alpha=lora_alpha)\n","            setattr(model, name, temp_lora)\n","        else:\n","            ignore_layers_str = [str(i) for i in ignore_layers]\n","            if name not in ignore_layers_str:\n","                add_lora_layers(module, module_names, r, lora_dropout, lora_alpha, ignore_layers)\n","\n","\n","def unfreeze_model(model):\n","    \"\"\"Unfreezes all parameters in a model by setting requires_grad to True.\"\"\"\n","    for name, param in model.named_parameters():\n","        param.requires_grad = True\n","\n","\n","def create_linear(module):\n","    \"\"\"Converts a LoRA linear module back to a linear module.\"\"\"\n","    k, d = module.pretrained.weight.shape  # pytorch nn.Linear weights are transposed, that is why variables are k, d and not d, k\n","    linear = nn.Linear(d, k, bias=True)\n","\n","    with torch.no_grad():\n","        linear.weight.copy_(module.pretrained.weight + (module.lora_B.weight @ module.lora_A.weight) * module.scaling)\n","        linear.bias.copy_(module.pretrained.bias)\n","\n","    return linear\n","\n","\n","def merge_lora_layers(model, module_names: Tuple=(\"query\", \"value\"), dropout=0.1):\n","    \"\"\"\n","        Replaces LoRA modules with their original linear equivalents.\n","\n","        Args:\n","            model: torch.nn.Module = The PyTorch model to be used\n","            module_names: Tuple = A tuple containing the names of the LoRA layers to replace\n","                Ex. (\"query\") to replace the LoRA modules with \"query\" in the name --> bert.encoder.layer.0.attention.self.query\n","            r: int =\n","            dropout: float = A float between 0 and 1 representing the dropout probability\n","        \"\"\"\n","    # enable dropout in frozen layers\n","    for module in model.modules():\n","        if isinstance(module, nn.Dropout):\n","            module.p = dropout\n","    # replace chosen linear modules with lora modules\n","    for name, module in model.named_children():\n","        if name in module_names and hasattr(module, \"pretrained\"):\n","            temp_linear = create_linear(module)\n","            setattr(model, name, temp_linear)\n","        else:\n","            merge_lora_layers(module, module_names=module_names, dropout=0.1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1714070486501,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"O0nPppmjWVdh","outputId":"204ad5ce-298d-499b-91fe-d52c89dd1f5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Name:  bert\n","Name:  embeddings\n","Name:  word_embeddings\n","Name:  position_embeddings\n","Name:  token_type_embeddings\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  encoder\n","Name:  layer\n","Name:  0\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  1\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  2\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  3\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  4\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  5\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  6\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  7\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  8\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  9\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  10\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  11\n","Name:  attention\n","Name:  self\n","Name:  query\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  key\n","Name:  value\n","Name:  lora_dropout\n","Name:  pretrained\n","Name:  lora_A\n","Name:  lora_B\n","Name:  dropout\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  intermediate\n","Name:  dense\n","Name:  intermediate_act_fn\n","Name:  output\n","Name:  dense\n","Name:  LayerNorm\n","Name:  dropout\n","Name:  pooler\n","Name:  dense\n","Name:  activation\n","Name:  dropout\n","Name:  classifier\n"]}],"source":["add_lora_layers(bert_base, r=1, lora_alpha=2)  # inject the LoRA layers into the model\n","freeze_model(bert_base)  # freeze the non-LoRA parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714070394404,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"q57CoCYqVOsu","outputId":"de33e9aa-3521-4e19-f44b-521bca9e19c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters: 109520642\n","Trainable parameters: 38402\n","Percentage trainable: 0.04%\n"]}],"source":["n_params = 0\n","n_trainable_params = 0\n","\n","# count the number of trainable parameters\n","for n, p in bert_base.named_parameters():\n","    n_params += p.numel()\n","    if p.requires_grad:\n","        n_trainable_params += p.numel()\n","\n","print(f\"Total parameters: {n_params}\")\n","print(f\"Trainable parameters: {n_trainable_params}\")\n","print(f\"Percentage trainable: {round(n_trainable_params / n_params * 100, 2)}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"elapsed":4,"status":"error","timestamp":1714070395853,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"mJ0iA_6TW_Fk","outputId":"3dcd9400-9c91-4b63-83c0-ab05951002fb"},"outputs":[{"ename":"NameError","evalue":"name 'train_iterator' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-1214a07d3514>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-04\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0meval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../models/bert_base_fine_tuned_lora_r8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_iterator' is not defined"]}],"source":["\n","#bert base lora all r = 8\n","trainer_bert_base_lora = BertTrainer(\n","    bert_base,\n","    tokenizer_base,\n","    lr=5e-04,\n","    epochs=10,\n","    train_dataloader=train_iterator,\n","    eval_dataloader=val_iterator,\n","    output_dir='../models/bert_base_fine_tuned_lora_r8',\n","    output_filename='bert_base_lora_r8',\n","    save=True,\n",")\n","\n","trainer_bert_base_lora.train(evaluate=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ar_c-Cy-MLfM"},"outputs":[],"source":["torch.save(trainer_bert_base_lora, \"./bert_lora_weights\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAji-56-PfF_"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"E-y2m6vqR1li"},"source":["# BERT w/"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["9931cd636df7428d98b86981427ad22e","6bf3b17b63e1458b968382a86ab8aaae","f41f9b395953404e829b0de705883933","ef1133aca00347af9c6af883e8603fc7","af62b95e0929442e8691af3f75849668","16571189681543e9b4f11b70f25568b3","82c5d5e955bf4ddc9ebfba4d42440148","0cceadc0671a47738d8b89421466d580","1eef1589c17e4831931c2abf7bad6d50","a86a49ce2d2e4e5191ac96b7fea98e53","5538076e5e87447381634105ae482d22","5e059af867d142cbb7987e6f35e9afd9","80e7acac90884bc8835d9223abfc2af6","fb465534d0f443dd8656c6c208ae16a9","6283293733064b82a13ad8b778052a4e","94d149aba9c44cb7a9f6df52ece6719a","a8305f39f7f745a9a9587326b2c58d00","3531bbdd90184a38b52d9d03c4d39dc3","15ef02168f8a42dd8304da23400e6f31","d76d7227626c4c27b7d8d33cd36f58cb","354b35281ad8418ca318507807279935","db3917f680b04ffab5168632ebd78efb"]},"executionInfo":{"elapsed":30075,"status":"ok","timestamp":1714149038235,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"s7hFmkf7BEqu","outputId":"0311d4ac-eba0-416a-80ac-be372b210d3d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9931cd636df7428d98b86981427ad22e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e059af867d142cbb7987e6f35e9afd9","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# load tokenizer and pretrained model\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# Load the latest version of the BERT model\n","bert_base = AutoModelForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels=2  # number of classes\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"elapsed":286,"status":"error","timestamp":1714149818247,"user":{"displayName":"Shri Kode","userId":"10872722213486344351"},"user_tz":240},"id":"B7qRER4IPft9","outputId":"eacb07b8-495a-4ec9-b618-57110deebdae"},"outputs":[],"source":["\n","checkpoint = torch.load('bert_lora_weights')\n","# model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EziO-XSDPjy5"},"outputs":[],"source":["bert_base.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4S7d93UPmbg"},"outputs":[],"source":["total_comp_time = 0\n","count = 0\n","for tokens, attention_masks, labels in val_iterator:\n","    # print(f'tokens: {tokens.shape}')\n","    # print(f'atteniton masks: {attention_masks.shape}')\n","    # print(f'labels: {labels.shape}')\n","    start = time.time()\n","    output = model(tokens)\n","    total_comp_time += time.time() - start\n","    print(total_comp_time)\n","    count += 1\n","print(count)\n","print(len(val_iterator))\n","print(\"Average Time: \", total_comp_time/len(val_iterator))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00f5c793f03d44afb7821246a925de3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04b030899804411e8553593be898d42c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bab052357df7406a9a44147cfe51b336","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e09470c15b264147bf3149261e66d920","value":440449768}},"0cceadc0671a47738d8b89421466d580":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"147a20d590ab4ddebea3284ecc977cf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15ef02168f8a42dd8304da23400e6f31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16571189681543e9b4f11b70f25568b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17d2127c8fb54cbc95d7ab54032f3ae5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4020a8bc1c4642f988819c984800f6c8","IPY_MODEL_4924d438bba44b18947194eb835192da","IPY_MODEL_b2d007d8190149308478b4b4ee80014f"],"layout":"IPY_MODEL_00f5c793f03d44afb7821246a925de3b"}},"1eef1589c17e4831931c2abf7bad6d50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"235d095e834d4e8da6aba14e3bdc0cce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50ba40d5e26d4d5591bf2403e01c1769","placeholder":"","style":"IPY_MODEL_e8523052537c4162ae8985fbc3c94713","value":"vocab.txt:100%"}},"29965c2ea66c483ab2faa6fdb8929567":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f284ca64fa0491ea26e2e53dacfbd17":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3531bbdd90184a38b52d9d03c4d39dc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"354b35281ad8418ca318507807279935":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a5cc069e13b4d289e42299aa65f728e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_529c27136cef407cb14f23c2844a497e","placeholder":"","style":"IPY_MODEL_449ce7f6f7624f9f8624dac9987e153c","value":"config.json:100%"}},"3ae3bba4c1a5495fa081a52bf25db3b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b2ed46290404a86af3d48eddd7c385b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4020a8bc1c4642f988819c984800f6c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89b9f8eb8b4742b2b18c050588ccb4a1","placeholder":"","style":"IPY_MODEL_bbe6ae8cdb5946c99f01df36266d2208","value":"tokenizer.json:100%"}},"449ce7f6f7624f9f8624dac9987e153c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4924d438bba44b18947194eb835192da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7410464a4a74296a2618025c26106ff","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfb6c7cfd2b94416bcc8e0d9c388380e","value":466062}},"4c62e005028e4533a165be4d6850b968":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d2cf59141474db0bbcc5bd873d0e56e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e08bf177d1a42daa8fb759408e1fd7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e2a903968f9482f835f132fcb12a5c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5046559880ed427b92dd39d18a2dd78b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50ba40d5e26d4d5591bf2403e01c1769":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"529c27136cef407cb14f23c2844a497e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52a7c9f4250b40879a43b063015dde20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5538076e5e87447381634105ae482d22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59a6c7d4c7674408a92828e7453223b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e059af867d142cbb7987e6f35e9afd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80e7acac90884bc8835d9223abfc2af6","IPY_MODEL_fb465534d0f443dd8656c6c208ae16a9","IPY_MODEL_6283293733064b82a13ad8b778052a4e"],"layout":"IPY_MODEL_94d149aba9c44cb7a9f6df52ece6719a"}},"6283293733064b82a13ad8b778052a4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_354b35281ad8418ca318507807279935","placeholder":"","style":"IPY_MODEL_db3917f680b04ffab5168632ebd78efb","value":"440M/440M[00:09&lt;00:00,25.3MB/s]"}},"6755e84ff3554f8193d8eaec816d6381":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"696c2b72bdd643f59cdf20f4bcafb5fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29965c2ea66c483ab2faa6fdb8929567","placeholder":"","style":"IPY_MODEL_52a7c9f4250b40879a43b063015dde20","value":"232k/232k[00:00&lt;00:00,14.9MB/s]"}},"6bf3b17b63e1458b968382a86ab8aaae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16571189681543e9b4f11b70f25568b3","placeholder":"","style":"IPY_MODEL_82c5d5e955bf4ddc9ebfba4d42440148","value":"config.json:100%"}},"71275f1cc8894952af1fa9c502bb241c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"759eed341c9b4779a787bfb24f2216b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a3ff827a22840aa86db87e6f6a8daf6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_235d095e834d4e8da6aba14e3bdc0cce","IPY_MODEL_af3f762a199841a3a8b971c1e7cefad9","IPY_MODEL_696c2b72bdd643f59cdf20f4bcafb5fc"],"layout":"IPY_MODEL_9a37b23a152b43408ead3c5afd28a8a5"}},"7b89194772174c81b8266fbdb5d25b00":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d2cf59141474db0bbcc5bd873d0e56e","placeholder":"","style":"IPY_MODEL_3b2ed46290404a86af3d48eddd7c385b","value":"570/570[00:00&lt;00:00,54.2kB/s]"}},"80e7acac90884bc8835d9223abfc2af6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8305f39f7f745a9a9587326b2c58d00","placeholder":"","style":"IPY_MODEL_3531bbdd90184a38b52d9d03c4d39dc3","value":"model.safetensors:100%"}},"8228cb05b61349fba8e263bd46125b46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82c5d5e955bf4ddc9ebfba4d42440148":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89b9f8eb8b4742b2b18c050588ccb4a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"942ac2e220ae4fe4986c1d8dbac0c4d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3cfe886b0b540328f20d692480e18ff","IPY_MODEL_04b030899804411e8553593be898d42c","IPY_MODEL_9ad4645a0f554415929d72273dbf418b"],"layout":"IPY_MODEL_4c62e005028e4533a165be4d6850b968"}},"94d149aba9c44cb7a9f6df52ece6719a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9931cd636df7428d98b86981427ad22e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6bf3b17b63e1458b968382a86ab8aaae","IPY_MODEL_f41f9b395953404e829b0de705883933","IPY_MODEL_ef1133aca00347af9c6af883e8603fc7"],"layout":"IPY_MODEL_af62b95e0929442e8691af3f75849668"}},"9a37b23a152b43408ead3c5afd28a8a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ad4645a0f554415929d72273dbf418b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e2a903968f9482f835f132fcb12a5c2","placeholder":"","style":"IPY_MODEL_a1ab21cf88aa49e38380735b2003057a","value":"440M/440M[00:01&lt;00:00,496MB/s]"}},"a1ab21cf88aa49e38380735b2003057a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3cfe886b0b540328f20d692480e18ff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6755e84ff3554f8193d8eaec816d6381","placeholder":"","style":"IPY_MODEL_4e08bf177d1a42daa8fb759408e1fd7f","value":"model.safetensors:100%"}},"a8305f39f7f745a9a9587326b2c58d00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a86a49ce2d2e4e5191ac96b7fea98e53":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9f7feecf09a4a5d972a622eaca64efa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adda16a85416431ea34948a3675c3458":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71275f1cc8894952af1fa9c502bb241c","placeholder":"","style":"IPY_MODEL_8228cb05b61349fba8e263bd46125b46","value":"tokenizer_config.json:100%"}},"af3f762a199841a3a8b971c1e7cefad9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59a6c7d4c7674408a92828e7453223b3","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf4c05beb60140359462d2f4707aaf98","value":231508}},"af62b95e0929442e8691af3f75849668":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2d007d8190149308478b4b4ee80014f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9f7feecf09a4a5d972a622eaca64efa","placeholder":"","style":"IPY_MODEL_759eed341c9b4779a787bfb24f2216b3","value":"466k/466k[00:00&lt;00:00,1.42MB/s]"}},"b41b1a04798c4bc7b85c485d297fd7ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b72d61e9af1a40968d25b40a1ce1b4f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_adda16a85416431ea34948a3675c3458","IPY_MODEL_ed44ba274a3441c094f1fb120f9e730e","IPY_MODEL_e332c33d5cb8423d8b08b7dd51f79f9f"],"layout":"IPY_MODEL_b41b1a04798c4bc7b85c485d297fd7ed"}},"bab052357df7406a9a44147cfe51b336":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbe6ae8cdb5946c99f01df36266d2208":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf4c05beb60140359462d2f4707aaf98":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c526e7c0a5c2492b986b9f63dde601de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d09164bce11c4bb0b267c6e5b5adfba6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a5cc069e13b4d289e42299aa65f728e","IPY_MODEL_e931a1c4773d41d3aa8d42807c639969","IPY_MODEL_7b89194772174c81b8266fbdb5d25b00"],"layout":"IPY_MODEL_c526e7c0a5c2492b986b9f63dde601de"}},"d7410464a4a74296a2618025c26106ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d76d7227626c4c27b7d8d33cd36f58cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9446a77daff4c138986acd027bc680c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db3917f680b04ffab5168632ebd78efb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfb6c7cfd2b94416bcc8e0d9c388380e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e09470c15b264147bf3149261e66d920":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e332c33d5cb8423d8b08b7dd51f79f9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f284ca64fa0491ea26e2e53dacfbd17","placeholder":"","style":"IPY_MODEL_147a20d590ab4ddebea3284ecc977cf0","value":"48.0/48.0[00:00&lt;00:00,4.32kB/s]"}},"e8523052537c4162ae8985fbc3c94713":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e931a1c4773d41d3aa8d42807c639969":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ae3bba4c1a5495fa081a52bf25db3b3","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed063d78c3cd47f7a0c3594b9a4cda70","value":570}},"ed063d78c3cd47f7a0c3594b9a4cda70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed44ba274a3441c094f1fb120f9e730e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5046559880ed427b92dd39d18a2dd78b","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d9446a77daff4c138986acd027bc680c","value":48}},"ef1133aca00347af9c6af883e8603fc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a86a49ce2d2e4e5191ac96b7fea98e53","placeholder":"","style":"IPY_MODEL_5538076e5e87447381634105ae482d22","value":"570/570[00:00&lt;00:00,7.14kB/s]"}},"f41f9b395953404e829b0de705883933":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cceadc0671a47738d8b89421466d580","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1eef1589c17e4831931c2abf7bad6d50","value":570}},"fb465534d0f443dd8656c6c208ae16a9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15ef02168f8a42dd8304da23400e6f31","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d76d7227626c4c27b7d8d33cd36f58cb","value":440449768}}}}},"nbformat":4,"nbformat_minor":0}
