{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligent Web Application Firewall\n",
    "====================================\n",
    "This project is based on data from the ECML/PKDD 2007 Challenge and CSIC 2010 Dataset, [available on GitHub](https://github.com/msudol/Web-Application-Attack-Datasets/blob/master/CSVData/csic_ecml_normalized_final.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies\n",
    "* [user-agents](https://pypi.org/project/user-agents/) is required to parse user agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip3 install pyyaml ua-parser user-agents\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure\n",
    "Initially setup this experiment. For the sake of making setup faster, we will go ahead and download the dataset if it does not already exist on our system. For now, we'll download it off of [msudol/Web-Application-Attack-Datasets](https://github.com/msudol/Web-Application-Attack-Datasets) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://raw.githubusercontent.com/msudol/Web-Application-Attack-Datasets/master/CSVData/csic_ecml_normalized_final.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enabling `DEBUG` will make a lot of messages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Data Caching\n",
    "We're going to cache the pre-processed data to avoid having to process it repeatedly. Set the `NORMALIZATION_VERSION` below accordingly to use a particular version of the normalized dataset. If it is set to `None`, we'll assume that pre-processing is still a work in progress and to continuously do pre-processing again (you should probably change this eventually). Note that this value will get automatically overridden if set to `None`, so you'll need to run these blocks again if you want data to be pre-processed again!\n",
    "\n",
    "Note that even if there's a cached version available, we'll always keep a version of the original data (kinda just for the sake of it).\n",
    "\n",
    "Also, if you *really* hate caching, just turn off saving..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZATION_VERSION = None\n",
    "SAVE_NORMALIZED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get things done! This code block will automatically download the dataset if needed and set up our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[~] Creating Dataset Directory\n",
      "[~] Creating Normalized Dataset Directory\n",
      "[~] Downloading Dataset\n",
      "[✅] Dataset Available at dataset/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import urllib.request\n",
    "\n",
    "DATASET_DIRECTORY_PATH = pathlib.Path('dataset')\n",
    "DATASET_NORMALIZED_DIRECTORY_PATH = DATASET_DIRECTORY_PATH.joinpath('normalized')\n",
    "DATASET_RAW_PATH = DATASET_DIRECTORY_PATH.joinpath('dataset.csv')\n",
    "\n",
    "\n",
    "if DATASET_DIRECTORY_PATH.is_dir():\n",
    "    print('[✅] Dataset Directory Exists')\n",
    "else:\n",
    "    print('[~] Creating Dataset Directory')\n",
    "    DATASET_DIRECTORY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DATASET_NORMALIZED_DIRECTORY_PATH.is_dir():\n",
    "    print('[✅] Normalized Dataset Directory Exists')\n",
    "else:\n",
    "    print('[~] Creating Normalized Dataset Directory')\n",
    "    DATASET_NORMALIZED_DIRECTORY_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if DATASET_RAW_PATH.is_file():\n",
    "    print('[✅] Dataset Exists')\n",
    "else:\n",
    "    print(f'[~] Downloading Dataset')\n",
    "    urllib.request.urlretrieve(DATASET_URL, DATASET_RAW_PATH)\n",
    "\n",
    "    if DATASET_RAW_PATH.is_file():\n",
    "        print(f'[✅] Dataset Available at {DATASET_RAW_PATH}')\n",
    "    else:\n",
    "        print('[⚠️] Failed to Download Dataset')\n",
    "        raise SystemExit('Dataset Download Failure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few quick functions to help us out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(message):\n",
    "    if DEBUG:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets figure out our caching situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_cache_path: pathlib.Path = None\n",
    "if NORMALIZATION_VERSION is None:\n",
    "    # Figure out the next normalization version...\n",
    "    i = 0\n",
    "    while True:\n",
    "        proposed_path = DATASET_NORMALIZED_DIRECTORY_PATH.joinpath(f'{i}.csv')\n",
    "        if not proposed_path.exists():\n",
    "            effective_cache_path = proposed_path\n",
    "            NORMALIZATION_VERSION = i\n",
    "            break\n",
    "\n",
    "        i += 1\n",
    "else:\n",
    "    effective_cache_path = DATASET_NORMALIZED_DIRECTORY_PATH.joinpath(f'{int(NORMALIZATION_VERSION)}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching path has been figured out! Time to actually normalize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENUMERATIONS = {\n",
    "    'Class': {'Valid': 0, 'Anomalous': 1},\n",
    "    'Method': {'GET': 0, 'POST': 1, 'PUT': 2},\n",
    "    'Host-Header': {'HTTP/1.0': 0, 'HTTP/1.1': 1},\n",
    "    'Connection': {'keep-alive': 0, 'close': 1, 'invalid': 2, None: 3},\n",
    "    'Pragma': {'no-cache': 0, 'invalid': 1, None: 2},\n",
    "    'Content-Type': {'application/x-www-form-urlencoded': 0, None: 1}\n",
    "}\n",
    "\n",
    "LENGTH_FIELDS = {\n",
    "    'Accept': 'Accept-Length',\n",
    "    'Accept-Charset': 'Accept-Charset-Length',\n",
    "    'Accept-Language': 'Accept-Language-Length',\n",
    "    'User-Agent': 'User-Agent-Length',\n",
    "    'Content-Type': 'Content-Type-Length',\n",
    "    'POST-Data': 'POST-Data-Length',\n",
    "    'GET-Query': 'GET-Query-Length'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "def enumerate(df):\n",
    "    for field, enumeration in ENUMERATIONS.items():\n",
    "        debug(f'[~] Processing Field: {field}')\n",
    "        if field not in df:\n",
    "            raise RuntimeWarning(f'Field {field} Does Not Exist')\n",
    "        \n",
    "        unenumerated_values = set(df[field].unique()).difference(set(enumeration.keys()))\n",
    "        for unenumerated_value in unenumerated_values:\n",
    "            if math.isnan(unenumerated_value) and None in enumeration:\n",
    "                continue\n",
    "            \n",
    "            raise RuntimeWarning(f'Failed to Enumerate Value \"{unenumerated_value}\" for Field {field}')\n",
    "        \n",
    "        if None in enumeration:\n",
    "            df[field] = df[field].map(enumeration).fillna(enumeration[None]).astype(int)\n",
    "        else:\n",
    "            df[field] = df[field].map(enumeration).astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def length_append(df):\n",
    "    for field, target in LENGTH_FIELDS.items():\n",
    "        debug(f'[~] Processing Field: {field}')\n",
    "        if field not in df:\n",
    "            raise RuntimeWarning(f'Field {field} Does Not Exist')\n",
    "        \n",
    "        df[target] = df[field].map(lambda v: len(v) if type(v) is str else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def __query_param_count(df):\n",
    "    df['GET-Query-Params'] = df['GET-Query'].map(lambda q: len(parse_qs(q).keys()) if type(q) is str else 0)\n",
    "\n",
    "def __query_characters(df):\n",
    "    CHARACTERS = [chr(o) for o in range(32, 127)]\n",
    "    for letter in CHARACTERS:\n",
    "        df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
    "\n",
    "def parse_query(df):\n",
    "    __query_param_count(df)\n",
    "    __query_characters(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(path):\n",
    "    df = parse_query(length_append(enumerate(pd.read_csv(path))))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[~] Processing Field: Class\n",
      "[~] Processing Field: Method\n",
      "[~] Processing Field: Host-Header\n",
      "[~] Processing Field: Connection\n",
      "[~] Processing Field: Pragma\n",
      "[~] Processing Field: Content-Type\n",
      "[~] Processing Field: Accept\n",
      "[~] Processing Field: Accept-Charset\n",
      "[~] Processing Field: Accept-Language\n",
      "[~] Processing Field: User-Agent\n",
      "[~] Processing Field: Content-Type\n",
      "[~] Processing Field: POST-Data\n",
      "[~] Processing Field: GET-Query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n",
      "/var/folders/y4/h92v27td7m1_v78ms19qrfvw0000gn/T/ipykernel_30944/3263872360.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'Letter-Frequency-{letter}'] = df['GET-Query'].apply(lambda q: q.count(letter) if type(q) is str else 0)\n"
     ]
    }
   ],
   "source": [
    "data: pd.DataFrame = None\n",
    "if effective_cache_path.is_file():\n",
    "    data = pd.read_csv(effective_cache_path)\n",
    "else:\n",
    "    # We're going to need to do some pre-processing!\n",
    "    data = preprocess(DATASET_RAW_PATH)\n",
    "\n",
    "    if SAVE_NORMALIZED:\n",
    "        data.to_csv(effective_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data:\n",
      "Class                                            1\n",
      "Method                                           0\n",
      "Host-Header                                      0\n",
      "Connection                                       2\n",
      "Accept                audio/*;q=0.7, audio/*;q=0.0\n",
      "                                  ...             \n",
      "Letter-Frequency-z                               3\n",
      "Letter-Frequency-{                               0\n",
      "Letter-Frequency-|                               0\n",
      "Letter-Frequency-}                               0\n",
      "Letter-Frequency-~                               0\n",
      "Name: 0, Length: 116, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    debug('Sample Data:')\n",
    "    debug(data.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we can enumerate even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept-Language May Be Enumerated with 14 Unique Values\n",
      "\t*;q=0.2, *;q=0.8, non-standard, nan, *;q=0.4, *;q=0.9, *;q=0.5, *, *;q=0.1, *;q=0.6, en, *;q=0.0, *;q=0.7, *;q=0.3\n",
      "\n",
      "[⚠️] Enumeration May be Incomplete: 1 May Be Enumerated\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ENUM_THRESHOLD = 64\n",
    "enumerable = 0\n",
    "for field in data:\n",
    "    values = data[field].unique()\n",
    "    if len(values) < ENUM_THRESHOLD and len({value for value in values if type(value) != np.int64}) > 0:\n",
    "        print(f'{field} May Be Enumerated with {len(values)} Unique Values')\n",
    "        values_as_str = ', '.join({str(value) for value in values})\n",
    "        print(f'\\t{values_as_str}')\n",
    "        enumerable += 1\n",
    "\n",
    "print()\n",
    "if enumerable > 0:\n",
    "    print(f'[⚠️] Enumeration May be Incomplete: {enumerable} May Be Enumerated')\n",
    "else:\n",
    "    print(f'[✅] Sufficiently Enumerated with Threshold {ENUM_THRESHOLD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CSICDataset, Vocab\n",
    "\n",
    "d = CSICDataset(csv_path='./dataset/dataset.csv', vocab_size=1000, min_frequency=2, tokenization_algorithm='vocab_map')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
